{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '.venv (Python 3.12.7)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from mirascope import llm, prompt_template\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "load_dotenv()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the web and returns the summaries of top results.\n",
    "    \n",
    "    Args:\n",
    "        query: The search query to be executed.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the summaries of the top results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from brave import Brave\n",
    "        brave = Brave(api_key=os.getenv(\"BRAVE_API_KEY\"))\n",
    "        results = brave.search(q=query, count=10, raw=True)\n",
    "        web_results = results.get(\"web\", {}).get(\"results\", [])\n",
    "        \n",
    "        summaries = []\n",
    "        for result in web_results:\n",
    "            if 'profile' not in result:\n",
    "                continue\n",
    "            url = result['url']\n",
    "            header = f\"{result['profile']['name']} - {result['profile']['long_name']}\"\n",
    "            title = result['title']\n",
    "            snippet = result['description']\n",
    "            summaries.append(f\"{header}\\n{title}\\n{snippet}\\n{url}\")\n",
    "        return \"\\n\\n\".join(summaries)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching the web: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub - github.com\n",
      "GitHub - Agent-RL/ReCall: ReCall: Learning to Reason with Tool Call for LLMs via Reinforcement Learning\n",
      "We introduce <strong>ReCall</strong>, a novel framework that trains LLMs to Reason with Tool Call via reinforcement learning—without requiring any supervised data on tool use trajectories or reasoning steps. <strong>ReCall</strong> empowers LLMs to agentically use and combine arbitrary tools like OpenAI o3, offering an accessible ...\n",
      "https://github.com/Agent-RL/ReCall\n",
      "\n",
      "Freecodecamp - freecodecamp.org\n",
      "The Agentic AI Handbook: A Beginner's Guide to Autonomous Intelligent Agents\n",
      "You may have heard about “Agentic AI” systems and wondered what they’re all about. Well, in basic terms, the idea behind Agentic AI is that it can see its surroundings, set and pursue goals, plan and reason through many processes, and learn from expe...\n",
      "https://www.freecodecamp.org/news/the-agentic-ai-handbook/\n",
      "\n",
      "Sebastianraschka - sebastianraschka.com\n",
      "The State of Reinforcement Learning for LLM Reasoning\n",
      "I&#x27;m an LLM <strong>Research</strong> Engineer with over a decade of experience in artificial intelligence. My work bridges academia and industry, with roles including senior staff at an AI company and a statistics professor. My expertise lies in LLM <strong>research</strong> and the development of high-performance AI systems, ...\n",
      "https://sebastianraschka.com/blog/2025/the-state-of-reinforcement-learning-for-llm-reasoning.html\n",
      "\n",
      "Bespokelabs - bespokelabs.ai\n",
      "Improving Multi-Turn Tool Use with Reinforcement Learning\n",
      "Recently, OpenAI has demonstrated that <strong>RL can be used to train a research agent that uses tools to carry out complex, multi-step workflows</strong>. However, they did not disclose a lot of details about their training recipe.\n",
      "https://www.bespokelabs.ai/blog/improving-multi-turn-tool-use-with-reinforcement-learning\n",
      "\n",
      "Scienceofaiagents - scienceofaiagents.com\n",
      "To keep doing RL research, stop calling yourself an RL researcher\n",
      "On the role of <strong>RL</strong> <strong>researchers</strong> in the era of LLM agents.\n",
      "https://www.scienceofaiagents.com/p/to-keep-doing-rl-research-stop-calling\n",
      "\n",
      "Medium - medium.com\n",
      "Tool Calling for LLMs: A Detailed Tutorial | by Explore With Yasir | Medium\n",
      "Tool calling <strong>allows an LLM to interact with external tools during conversation generation</strong>. This means the LLM can access information or perform actions beyond its own knowledge base.\n",
      "https://medium.com/@developer.yasir.pk/tool-calling-for-llms-a-detailed-tutorial-a2b4d78633e2\n",
      "\n",
      "ArXiv - arxiv.org\n",
      "OTC: Optimal Tool Calls via Reinforcement Learning\n",
      "In this work, we propose <strong>Optimal Tool Call-controlled Policy Optimization (OTC-PO),</strong> a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool ...\n",
      "https://arxiv.org/html/2504.14870v1\n",
      "\n",
      "Reddit - reddit.com\n",
      "r/LocalLLaMA on Reddit: How does tool calling work?\n",
      "I was looking at a bunch of different models today after installing Ollama, and because they all support this OpenAI-compatible API, a lot of them…\n",
      "https://www.reddit.com/r/LocalLLaMA/comments/1iywmss/how_does_tool_calling_work/\n",
      "\n",
      "Huggingface - huggingface.co\n",
      "Paper page - OTC: Optimal Tool Calls via Reinforcement Learning\n",
      "In this work, we propose <strong>Optimal Tool Call-controlled Policy Optimization (OTC-PO),</strong> a simple yet effective RL-based framework that encourages models to produce accurate answers with minimal tool calls. Our method introduces a tool-integrated reward that jointly considers correctness and tool ...\n",
      "https://huggingface.co/papers/2504.14870\n",
      "\n",
      "Hacker News - news.ycombinator.com\n",
      "Search-R1: Training LLMs to Reason and Leverage Search Engines with RL | Hacker News\n",
      "When OpenAI was launched this is what I thought it was going to be like. Something, something for the betterment of man kind · The good thing is that there are a lot of companies out there that want to make a name for themselves. Mistral started like that with Apache 2.0 models, now ds w/ ...\n",
      "https://news.ycombinator.com/item?id=43563265\n"
     ]
    }
   ],
   "source": [
    "# testing the search function\n",
    "result = web_search(\"what is the latest research on RL on tool calling?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolRL: Reward is All Tool Learning Needs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. [1 Introduction](https://arxiv.org/html/2504.13958v1#S1 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "2. [2 Related Work](https://arxiv.org/html/2504.13958v1#S2 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "   1. [Tool-Integrated Reasoning of LLMs.](https://arxiv.org/html/2504.13958v1#S2.SS0.SSS0.Px1 \"In 2 Related Work ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   2. [Exploration of RL in LLMs.](https://arxiv.org/html/2504.13958v1#S2.SS0.SSS0.Px2 \"In 2 Related Work ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "3. [3 Method](https://arxiv.org/html/2504.13958v1#S3 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "   1. [3.1 Task Definition](https://arxiv.org/html/2504.13958v1#S3.SS1 \"In 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   2. [3.2 TIR Rollout](https://arxiv.org/html/2504.13958v1#S3.SS2 \"In 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   3. [3.3 Reward Design](https://arxiv.org/html/2504.13958v1#S3.SS3 \"In 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Format Reward.](https://arxiv.org/html/2504.13958v1#S3.SS3.SSS0.Px1 \"In 3.3 Reward Design ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      2. [Correctness Reward.](https://arxiv.org/html/2504.13958v1#S3.SS3.SSS0.Px2 \"In 3.3 Reward Design ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   4. [3.4 RL Training with GRPO](https://arxiv.org/html/2504.13958v1#S3.SS4 \"In 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Normalized Advantage Across Query Groups.](https://arxiv.org/html/2504.13958v1#S3.SS4.SSS0.Px1 \"In 3.4 RL Training with GRPO ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      2. [Policy Optimization Objective.](https://arxiv.org/html/2504.13958v1#S3.SS4.SSS0.Px2 \"In 3.4 RL Training with GRPO ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "4. [4 Experiments](https://arxiv.org/html/2504.13958v1#S4 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "   1. [4.1 Training Dataset](https://arxiv.org/html/2504.13958v1#S4.SS1 \"In 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   2. [4.2 Experiment Settings](https://arxiv.org/html/2504.13958v1#S4.SS2 \"In 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Training.](https://arxiv.org/html/2504.13958v1#S4.SS2.SSS0.Px1 \"In 4.2 Experiment Settings ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      2. [Evaluation.](https://arxiv.org/html/2504.13958v1#S4.SS2.SSS0.Px2 \"In 4.2 Experiment Settings ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      3. [Baselines.](https://arxiv.org/html/2504.13958v1#S4.SS2.SSS0.Px3 \"In 4.2 Experiment Settings ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   3. [4.3 Results](https://arxiv.org/html/2504.13958v1#S4.SS3 \"In 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Main Results.](https://arxiv.org/html/2504.13958v1#S4.SS3.SSS0.Px1 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      2. [SFT Initialization Impacts.](https://arxiv.org/html/2504.13958v1#S4.SS3.SSS0.Px2 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      3. [Reward Design on PPO.](https://arxiv.org/html/2504.13958v1#S4.SS3.SSS0.Px3 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      4. [Generalization Studies.](https://arxiv.org/html/2504.13958v1#S4.SS3.SSS0.Px4 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      5. [Free-form Inference Effectiveness.](https://arxiv.org/html/2504.13958v1#S4.SS3.SSS0.Px5 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "5. [5 Analysis](https://arxiv.org/html/2504.13958v1#S5 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "   1. [5.1 Effect of Length Reward](https://arxiv.org/html/2504.13958v1#S5.SS1 \"In 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Dynamic Length Reward.](https://arxiv.org/html/2504.13958v1#S5.SS1.SSS0.Px1 \"In 5.1 Effect of Length Reward ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   2. [5.2 Effect of Reward Scale](https://arxiv.org/html/2504.13958v1#S5.SS2 \"In 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "      1. [Dynamic Reward Scaling.](https://arxiv.org/html/2504.13958v1#S5.SS2.SSS0.Px1 \"In 5.2 Effect of Reward Scale ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   3. [5.3 Effect of Reward Granularity](https://arxiv.org/html/2504.13958v1#S5.SS3 \"In 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "6. [6 Conclusion](https://arxiv.org/html/2504.13958v1#S6 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "7. [A User Prompt Details](https://arxiv.org/html/2504.13958v1#A1 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "8. [B Experiment Details](https://arxiv.org/html/2504.13958v1#A2 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "   1. [Training Data Details.](https://arxiv.org/html/2504.13958v1#A2.SS0.SSS0.Px1 \"In Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   2. [GRPO Setting Details.](https://arxiv.org/html/2504.13958v1#A2.SS0.SSS0.Px2 \"In Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   3. [Baselines.](https://arxiv.org/html/2504.13958v1#A2.SS0.SSS0.Px3 \"In Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "   4. [PPO Setting Details.](https://arxiv.org/html/2504.13958v1#A2.SS0.SSS0.Px4 \"In Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\")\n",
      "9. [C Additional Results](https://arxiv.org/html/2504.13958v1#A3 \"In ToolRL: Reward is All Tool Learning Needs\")\n",
      "\n",
      "ToolRL: Reward is All Tool Learning Needs\n",
      "=========================================\n",
      "\n",
      "Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen,\n",
      "  \n",
      "Dilek Hakkani-Tür, Gokhan Tur, Heng Ji\n",
      "  \n",
      "University of Illinois Urbana-Champaign\n",
      "  \n",
      "{chengq9, hengji}@illinois.edu\n",
      "\n",
      "###### Abstract\n",
      "\n",
      "Current Large Language Models (LLMs) often undergo supervised fine-tuning (SFT) to acquire tool use capabilities. However, SFT struggles to generalize to unfamiliar or complex tool use scenarios. Recent advancements in reinforcement learning (RL), particularly with R1-like models, have demonstrated promising reasoning and generalization abilities. Yet, reward design for tool use presents unique challenges: multiple tools may be invoked with diverse parameters, and coarse-grained reward signals, such as answer matching, fail to offer the finegrained feedback required for effective learning.\n",
      "In this work, we present the first comprehensive study on reward design for tool selection and application tasks within the RL paradigm. We systematically explore a wide range of reward strategies, analyzing their types, scales, granularity, and temporal dynamics. Building on these insights, we propose a principled reward design tailored for tool use tasks and apply it to train LLMs using Group Relative Policy Optimization (GRPO).\n",
      "Empirical evaluations across diverse benchmarks demonstrate that our approach yields robust, scalable, and stable training, achieving a 17% improvement over base models and a 15% gain over SFT models. These results highlight the critical role of thoughtful reward design in enhancing the tool use capabilities and generalization performance of LLMs. All the code are released to facilitate future research.111 Data and codes released at <https://github.com/qiancheng0/ToolRL>\n",
      "\n",
      "ToolRL: Reward is All Tool Learning Needs\n",
      "\n",
      "  \n",
      "\n",
      "Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen,\n",
      "\n",
      "Dilek Hakkani-Tür, Gokhan Tur, Heng Ji\n",
      "\n",
      "University of Illinois Urbana-Champaign\n",
      "\n",
      "{chengq9, hengji}@illinois.edu\n",
      "\n",
      "1 Introduction\n",
      "--------------\n",
      "\n",
      "![Refer to caption](extracted/6367558/figures/introduction.png)\n",
      "\n",
      "\n",
      "Figure 1: SFT on distilled deep-thinking trajectories suffers from overthinking and limited generalization.\n",
      "\n",
      "![Refer to caption](x1.png)\n",
      "\n",
      "\n",
      "Figure 2: Main results (left) and reward trends over training steps for GRPO Cold Start across four models (right). GRPO Cold Start, equipped with our proposed reward design, consistently achieves the highest performance, with reward curves showing a rapid increase during training.\n",
      "\n",
      "Recent advances in Large Language Models (LLMs) have showcased remarkable capabilities in complex reasoning tasks (Kumar et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib18)). Among the techniques that have significantly contributed to this progress, Reinforcement Learning (RL) has emerged as a powerful paradigm, enabling LLMs to develop emergent capabilities such as self-reflection, self-correction, and long-horizon planning (Guo et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib11); Team et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib46)). These capabilities have been instrumental in the success of models like o1 and R1, particularly in mathematical and logical reasoning domains (Qin et al., [2024a](https://arxiv.org/html/2504.13958v1#bib.bib34); Huang et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib13); Li et al., [2025b](https://arxiv.org/html/2504.13958v1#bib.bib21); Kang et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib16)).\n",
      "\n",
      "Beyond traditional reasoning tasks, an increasingly important area is Tool-Integrated Reasoning (TIR). TIR involves LLMs interacting with external tools, such as search engines (Jin et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib15); Zheng et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib60)), calculators (Chen et al., [2023b](https://arxiv.org/html/2504.13958v1#bib.bib4); Qin et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib35)), or code interpreters (Gou et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib10); Liao et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib22)), in a multi-step, feedback-driven loop to arrive at solutions. TIR is particularly important because it addresses core limitations of LLMs, such as outdated knowledge, calculation inaccuracy, and shallow reasoning. By integrating external tools that offer real-time access and specialized capabilities, TIR enables models to tackle complex tasks in a more grounded and goal-directed way.\n",
      "\n",
      "Unlike textual reasoning, which primarily involves deduction and inference from static text, TIR additionally demands the model’s ability to select appropriate tools, interpret intermediate outputs, and adaptively refine its trajectory on the fly. These dynamic and interactive reasoning skills position TIR at the core of the emerging paradigm of LLMs-as-agents. As such, TIR enables a wide range of applications, including scientific discovery (Roohani et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib39); Inoue et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib14)), research automation (Baek et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib2); Wang et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib49)), embodied task completion (Zhang et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib58); Huang et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib12)), and everyday decision-making (Ye et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib52); Zhai et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib57)).\n",
      "\n",
      "Training LLMs for TIR tasks has predominantly relied on Supervised Fine-Tuning (SFT), wherein existing approaches typically generate these integrated reasoning steps offline, followed by subsequent SFT on these trajectories (Chen et al., [2023a](https://arxiv.org/html/2504.13958v1#bib.bib3); Zeng et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib56); Chen et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib6); Acikgoz et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib1)). While SFT is effective to some extent, it struggles with generalization, exploration, and adaptability (Chu et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib7); Guo et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib11)). As illustrated in [Figure 1](https://arxiv.org/html/2504.13958v1#S1.F1 \"In 1 Introduction ‣ ToolRL: Reward is All Tool Learning Needs\"), a model trained with SFT on deep-thinking trajectories over-interprets the tool and fails to reject the inappropriate tool, merely imitating cues like “but wait” without engaging in genuine deep thinking. As such, SFT often fails to capture the strategic flexibility needed for optimal tool use, particularly in open-ended or multi-step settings. This motivates a fundamental research question: Can RL-based training methods better equip LLMs with agentic tool-using capabilities, and if so, what is the optimal RL design for TIR?\n",
      "\n",
      "Recent efforts such as Search-R1 (Jin et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib15)) and TORL (Li et al., [2025b](https://arxiv.org/html/2504.13958v1#bib.bib21)) have begun to explore this direction. However, their focus is narrow: either constrained to search tools in question answering settings or code tools in math problem-solving. In contrast, our work aims to study RL-based training for general-purpose tool selection and application, across diverse and complex tool sets with different task types.\n",
      "\n",
      "For an RL algorithm to be effective, a well-designed reward is essential. Unlike math tasks with a single correct answer, Tool-Integrated Reasoning (TIR) tasks introduce multiple layers of complexity: they often involve multi-step interactions where each turn may require invoking multiple tools, each with carefully specified parameters. Designing effective reward signals to guide learning through this complexity remains an open and underexplored challenge. In this paper, we focus on the problem of reward design for TIR and propose a principled, generalizable framework that can be applied across various RL algorithms. While our reward design is algorithm-agnostic by nature, we empirically demonstrate its effectiveness using both Group Relative Policy Optimization (GRPO) (Shao et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib42)) and Proximal Policy Optimization (PPO) (Schulman et al., [2017](https://arxiv.org/html/2504.13958v1#bib.bib41)), showcasing its versatility and impact on improving tool use performance.\n",
      "\n",
      "We begin by formalizing the TIR task, and outlining general principles for effective reward design. Building on this foundation, we show how RL algorithm can be leveraged to train LLMs for robust and context-aware tool selection and application. Empirical results demonstrate that our approach outperforms base models by 17% and SFT models by 15% across multiple tool use and QA benchmarks. Moreover, the trained model exhibits strong generalization to unseen scenarios and task objectives, along with emergent behaviors such as proactiveness and metacognitive reasoning.\n",
      "\n",
      "To identify optimal reward strategies, we next systematically explore a broad spectrum of reward configurations across four key dimensions: (1) reward type (what aspect to reward), (2) reward scale (how much to reward), (3) reward granularity (how detailed the reward signal is), and (4) reward dynamics (how rewards evolve over time). Through extensive experiments, we identify reward designs that best align with agentic tool use and uncover insights into what makes a reward “useful” for tool invoking LLMs. We summarize the core insights we derive as follows:\n",
      "\n",
      "* •\n",
      "\n",
      "  Longer reasoning trace is not inherently better and length rewards can degrade performance.\n",
      "* •\n",
      "\n",
      "  Dynamic reward scale helps models transition smoothly from simple to complex behaviors.\n",
      "* •\n",
      "\n",
      "  Finegrained reward decomposition leads to more stable and effective learning.\n",
      "\n",
      "We also summarize the overall contributions of our paper as follows:\n",
      "\n",
      "* •\n",
      "\n",
      "  We present the first systematic study on RL-based training for general-purpose tool selection and application in LLMs.\n",
      "* •\n",
      "\n",
      "  We propose a principled reward design framework tailored for TIR and validate its effectiveness through RL algorithms including GRPO.\n",
      "* •\n",
      "\n",
      "  We conduct extensive experiments analyzing the effects of various reward strategies and distill actionable insights for future research on LLM-agent training.\n",
      "\n",
      "This work pioneers the application of RL to general TIR and provides the first empirical roadmap for reward design in TIR, paving the way toward more capable and autonomous LLM agents.\n",
      "\n",
      "2 Related Work\n",
      "--------------\n",
      "\n",
      "#### Tool-Integrated Reasoning of LLMs.\n",
      "\n",
      "Tool-integrated reasoning (TIR) has emerged as a promising approach to enhance the capabilities of LLMs. Early studies introduced the concept of equipping LLMs with external tools to overcome their inherent limitations Schick et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib40)); Qin et al. ([2024b](https://arxiv.org/html/2504.13958v1#bib.bib36)); Yao et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib51)), such as program executors Chen et al. ([2022](https://arxiv.org/html/2504.13958v1#bib.bib5)) and search engines Vu et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib48)). To systematically assess these enhanced capabilities, several benchmarks have been proposed to evaluate tool use performance across various dimensions, including API selection, argument generation, and generalization Qin et al. ([2024c](https://arxiv.org/html/2504.13958v1#bib.bib37)); Patil et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib27)); Qian et al. ([2024a](https://arxiv.org/html/2504.13958v1#bib.bib32)). Building on this foundation, subsequent research has focused on constructing high-quality tool use datasets Liu et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib25)); Qian et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib30)), enabling models to autonomously create and invoke tools Qian et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib31), [2024b](https://arxiv.org/html/2504.13958v1#bib.bib33)), and applying these techniques to problems spanning different modalities Shen et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib43)) and specialized domains Ling et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib24)). More recently, reinforcement learning (RL) has been explored as an effective framework to further improve TIR, demonstrating success in tasks such as information retrieval Jin et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib15)) and math computation Li et al. ([2025b](https://arxiv.org/html/2504.13958v1#bib.bib21)). These advances collectively highlight the growing potential of tool-augmented LLMs for general-purpose reasoning in open-domain settings.\n",
      "\n",
      "#### Exploration of RL in LLMs.\n",
      "\n",
      "Previous work has primarily relied on supervised fine-tuning (SFT) with carefully curated datasets to enhance LLM performance in tool use Schick et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib40)); Qin et al. ([2024c](https://arxiv.org/html/2504.13958v1#bib.bib37)). Recently, reinforcement learning (RL) has gained traction as a more scalable and generalizable training paradigm. The development of RL methods for LLMs has evolved from reinforcement learning from human feedback (RLHF) Kaufmann et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib17)) and proximal policy optimization (PPO) Schulman et al. ([2017](https://arxiv.org/html/2504.13958v1#bib.bib41)) to more advanced techniques such as direct preference optimization (DPO) Rafailov et al. ([2023](https://arxiv.org/html/2504.13958v1#bib.bib38)), SimPO Meng et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib26)), and group relative policy optimization (GRPO) Shao et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib42)). Extensions like dynamic sampling policy optimization (DAPO) Yu et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib53)) and the more recent value-based augmented proximal policy optimization (VAPO) Yuan et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib55)) further improve training stability and efficiency.\n",
      "\n",
      "Among these, GRPO Shao et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib42)) is specifically designed for LLMs, replacing the traditional critic with a group-based evaluation strategy. It has shown strong performance in enhancing reasoning abilities across a range of tasks, including mathematical problem solving Shao et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib42)); Xie et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib50)), search engine interaction Jin et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib15)); Song et al. ([2025](https://arxiv.org/html/2504.13958v1#bib.bib45)), and code generation Li et al. ([2025b](https://arxiv.org/html/2504.13958v1#bib.bib21)). Beyond task variety, recent studies have analyzed the influence of dataset scale Li et al. ([2025a](https://arxiv.org/html/2504.13958v1#bib.bib20)) and GRPO’s effectiveness in smaller model settings Dang and Ngo ([2025](https://arxiv.org/html/2504.13958v1#bib.bib8)). GRPO’s flexible reward function enables adaptation to diverse objectives, such as assigning weights to sub-tasks Yu et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib54)) or constraining tool use frequency Li et al. ([2025b](https://arxiv.org/html/2504.13958v1#bib.bib21)). In this work, we extend GRPO to enhance general tool use capabilities, improving LLMs’ ability to select and interact with external tools across a wide range of scenarios.\n",
      "\n",
      "3 Method\n",
      "--------\n",
      "\n",
      "Supervised fine-tuning (SFT), as illustrated in [Figure 1](https://arxiv.org/html/2504.13958v1#S1.F1 \"In 1 Introduction ‣ ToolRL: Reward is All Tool Learning Needs\"), often suffers from overfitting to certain patterns and constrains the model’s ability to learn optimal strategies for tool use. To address this, we introduce a reinforcement learning (RL) approach for enhancing tool-integrated reasoning (TIR) in LLMs. In this section, we begin by defining the TIR task ([Section 3.1](https://arxiv.org/html/2504.13958v1#S3.SS1 \"3.1 Task Definition ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")), followed by our customized rollout strategy ([Section 3.2](https://arxiv.org/html/2504.13958v1#S3.SS2 \"3.2 TIR Rollout ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")) and reward design ([Section 3.3](https://arxiv.org/html/2504.13958v1#S3.SS3 \"3.3 Reward Design ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")). These components are then integrated into the Group Relative Policy Optimization (GRPO) framework Shao et al. ([2024](https://arxiv.org/html/2504.13958v1#bib.bib42)) to guide model training on general TIR tasks ([Section 3.4](https://arxiv.org/html/2504.13958v1#S3.SS4 \"3.4 RL Training with GRPO ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\")).\n",
      "\n",
      "### 3.1 Task Definition\n",
      "\n",
      "Tool-Integrated Reasoning (TIR) is the process of incorporating external tools into the reasoning trajectory of an LLM to solve a user task. A typical TIR trajectory involves multiple tool invocations over several reasoning steps, with the final outcome determined by the cumulative success of these intermediate decisions.\n",
      "\n",
      "Formally, given a tool set 𝒯={t1,t2,…,tn}𝒯subscript𝑡1subscript𝑡2…subscript𝑡𝑛\\mathcal{T}=\\{t\\_{1},t\\_{2},\\ldots,t\\_{n}\\}caligraphic\\_T = { italic\\_t start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , italic\\_t start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , … , italic\\_t start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT } containing n𝑛nitalic\\_n available tools, and a user query Q𝑄Qitalic\\_Q, the reasoning trajectory up to step k𝑘kitalic\\_k is denoted as:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | sk=(r1,𝒯1,o1),(r2,𝒯2,o2),…,(rk,𝒯k,ok),subscript𝑠𝑘  subscript𝑟1subscript𝒯1subscript𝑜1subscript𝑟2subscript𝒯2subscript𝑜2…subscript𝑟𝑘subscript𝒯𝑘subscript𝑜𝑘s\\_{k}=\\left(r\\_{1},\\mathcal{T}\\_{1},o\\_{1}\\right),\\left(r\\_{2},\\mathcal{T}\\_{2},o\\_{% 2}\\right),\\ldots,\\left(r\\_{k},\\mathcal{T}\\_{k},o\\_{k}\\right),italic\\_s start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT = ( italic\\_r start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , italic\\_o start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) , ( italic\\_r start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , italic\\_o start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT ) , … , ( italic\\_r start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT , italic\\_o start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT ) , |  |\n",
      "\n",
      "where risubscript𝑟𝑖r\\_{i}italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT denotes the model’s natural language reasoning at step i𝑖iitalic\\_i, 𝒯i⊆𝒯subscript𝒯𝑖𝒯\\mathcal{T}\\_{i}\\subseteq\\mathcal{T}caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ⊆ caligraphic\\_T denotes the set of tool calls invoked at step i𝑖iitalic\\_i, and oisubscript𝑜𝑖o\\_{i}italic\\_o start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT denotes the observation received after executing tools in 𝒯isubscript𝒯𝑖\\mathcal{T}\\_{i}caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT, possibly including both environment and user feedback.\n",
      "\n",
      "At each step k+1𝑘1k+1italic\\_k + 1, the model must generate the next reasoning step rk+1subscript𝑟𝑘1r\\_{k+1}italic\\_r start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT, select a set of tools 𝒯k+1⊆𝒯subscript𝒯𝑘1𝒯\\mathcal{T}\\_{k+1}\\subseteq\\mathcal{T}caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT ⊆ caligraphic\\_T, and formulate a grounded tool call (i.e., a parameterized invocation of each tool) to make progress toward solving Q𝑄Qitalic\\_Q.\n",
      "\n",
      "The model’s policy is defined as π:sk→(rk+1,𝒯k+1):𝜋→subscript𝑠𝑘subscript𝑟𝑘1subscript𝒯𝑘1\\pi:s\\_{k}\\to\\left(r\\_{k+1},\\mathcal{T}\\_{k+1}\\right)italic\\_π : italic\\_s start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT → ( italic\\_r start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT ), where the model’s objective at each step is to select a tool set 𝒯k+1subscript𝒯𝑘1\\mathcal{T}\\_{k+1}caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT that maximizes the immediate reward:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | 𝒯k+1∗=arg⁡max𝒯k+1⊆𝒯⁡R⁢(sk,𝒯k+1,ok+1),superscriptsubscript𝒯𝑘1subscriptsubscript𝒯𝑘1𝒯𝑅subscript𝑠𝑘subscript𝒯𝑘1subscript𝑜𝑘1\\mathcal{T}\\_{k+1}^{\\*}=\\arg\\max\\_{\\mathcal{T}\\_{k+1}\\subseteq\\mathcal{T}}\\;R(s\\_{k% },\\mathcal{T}\\_{k+1},o\\_{k+1}),caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT ∗ end\\_POSTSUPERSCRIPT = roman\\_arg roman\\_max start\\_POSTSUBSCRIPT caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT ⊆ caligraphic\\_T end\\_POSTSUBSCRIPT italic\\_R ( italic\\_s start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT , italic\\_o start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT ) , |  |\n",
      "\n",
      "where R⁢(⋅)𝑅⋅R(\\cdot)italic\\_R ( ⋅ ) represents the reward function that evaluates progress made by invoking the tools in 𝒯k+1subscript𝒯𝑘1\\mathcal{T}\\_{k+1}caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT.\n",
      "\n",
      "While the immediate reward at each step is maximized, the model’s policy is implicitly optimized to maximize the cumulative reward over the entire trajectory, formulated as:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | maxπ⁡𝔼π⁢[∑k=1KR⁢(sk,𝒯k+1,ok+1)],subscript𝜋subscript𝔼𝜋delimited-[]superscriptsubscript𝑘1𝐾𝑅subscript𝑠𝑘subscript𝒯𝑘1subscript𝑜𝑘1\\max\\_{\\pi}\\;\\mathbb{E}\\_{\\pi}\\left[\\sum\\_{k=1}^{K}R(s\\_{k},\\mathcal{T}\\_{k+1},o\\_{k% +1})\\right],roman\\_max start\\_POSTSUBSCRIPT italic\\_π end\\_POSTSUBSCRIPT blackboard\\_E start\\_POSTSUBSCRIPT italic\\_π end\\_POSTSUBSCRIPT [ ∑ start\\_POSTSUBSCRIPT italic\\_k = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_K end\\_POSTSUPERSCRIPT italic\\_R ( italic\\_s start\\_POSTSUBSCRIPT italic\\_k end\\_POSTSUBSCRIPT , caligraphic\\_T start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT , italic\\_o start\\_POSTSUBSCRIPT italic\\_k + 1 end\\_POSTSUBSCRIPT ) ] , |  |\n",
      "\n",
      "This formulation is valid because our training data includes ground truth tool calls at each step, allowing step-wise reward signals to guide multi-step success. Unlike QA tasks that focus solely on the final answer, tool selection and application tasks provide dense intermediate feedback. Moreover, we later demonstrate that our method enables the model to generalize to settings where tool calls are free-form and only the final outcome matters. Therefore, out task setting encourages the model to optimize tool use at each step while aligning with the overall task goal.\n",
      "\n",
      "![Refer to caption](extracted/6367558/figures/reward.png)\n",
      "\n",
      "\n",
      "Figure 3: Illustration of TIR rollout and calculation of format and correctness reward.\n",
      "\n",
      "\n",
      "\n",
      "System Prompt for Training\n",
      "\n",
      "You are a helpful dialogue assistant capable of leveraging tool calls to solve user tasks and provide structured chat responses.\n",
      "Available Tools\n",
      "  \n",
      "In your response, you can use the following tools:\n",
      "{{Tool List}}\n",
      "Steps for Each Turn\n",
      "  \n",
      "1. Think: Recall relevant context and analyze the current user goal.\n",
      "2. Decide on Tool Usage: If a tool is needed, specify the tool and its parameters.\n",
      "3. Respond Appropriately: If a response is needed, generate one while maintaining consistency across user queries.\n",
      "Output Format\n",
      "  \n",
      "<think> Your thoughts and reasoning </think>\n",
      "<tool\\_call>\n",
      "{“name”: “Tool name”, “parameters”: {“Parameter name”: “Parameter content”, “… …”: “… …”}}\n",
      "{“name”: “… …”, “parameters”: {“… …”: “… …”, “… …”: “… …”}}\n",
      "…\n",
      "</tool\\_call>\n",
      "<response> AI’s final response </response>\n",
      "Important Notes\n",
      "  \n",
      "1. You must always include the <think> field to outline your reasoning. Provide at least one of <tool\\_call> or <response>. Decide whether to use <tool\\_call> (possibly multiple times), <response>, or both.\n",
      "2. You can invoke multiple tool calls simultaneously in the <tool\\_call> fields. Each tool call should be a JSON object with a “name” field and a “parameters” field containing a dictionary of parameters. If no parameters are needed, leave the “parameters” field an empty dictionary.\n",
      "3. Refer to the previous dialogue records in the history, including the user’s queries, previous <tool\\_call>, <response>, and any tool feedback noted as <obs> (if exists).\n",
      "\n",
      "Figure 4: The system prompt used for TIR’s rollout.\n",
      "\n",
      "### 3.2 TIR Rollout\n",
      "\n",
      "To enable the model to autonomously generate reasoning traces and tool calls, we utilize a system prompt as shown in [Figure 4](https://arxiv.org/html/2504.13958v1#S3.F4 \"In 3.1 Task Definition ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\") during rollout. The Tool List placeholder denotes the tool set 𝒯𝒯\\mathcal{T}caligraphic\\_T, which contains all tools available for invocation. We indicate in the instruction that the LLM should use special tokens <think>, <tool\\_call>, and <response> to indicates their thoughts, tool calls and responses in output.\n",
      "\n",
      "As illustrated in [Figure 3](https://arxiv.org/html/2504.13958v1#S3.F3 \"In 3.1 Task Definition ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\"), when the model output includes <tool\\_call>, we automatically parse the tool calls into individual invocations using the model-predicted parameters. The outputs from executions are then inserted into the <obs> field and appended to the dialogue history, whose format is shown in [Figure 11](https://arxiv.org/html/2504.13958v1#Ax1.F11 \"In Appendix ‣ ToolRL: Reward is All Tool Learning Needs\"), serving as the model’s interaction trajectory. Similarly, if the output contains <response>, the corresponding response is parsed and appended to the dialogue history.\n",
      "\n",
      "It is important to note that <tool\\_call> and <response> are not mutually exclusive; they may co-occur within a single output. The user’s initial query Q𝑄Qitalic\\_Q is placed in the Initial User Input placeholder, and any subsequent user inputs are also appended to the dialogue history when present.\n",
      "\n",
      "### 3.3 Reward Design\n",
      "\n",
      "Rule-based reward mechanisms have demonstrated strong empirical performance and are commonly employed. In our training, we similarly adopt a reward formulation that combines structural and correctness-based components, in line with prior works (Jin et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib15); Li et al., [2025b](https://arxiv.org/html/2504.13958v1#bib.bib21); Xie et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib50)). Specifically, the format reward assesses whether the model output adheres to the expected structure including thoughts, tool calls, and responses, while the correctness reward evaluates the accuracy of tool invocations. Formally, the overall reward Rfinal⁢(⋅)subscript𝑅final⋅R\\_{\\text{final}}(\\cdot)italic\\_R start\\_POSTSUBSCRIPT final end\\_POSTSUBSCRIPT ( ⋅ ) is decomposed into two components: Rformat+Rcorrectsubscript𝑅formatsubscript𝑅correctR\\_{\\text{format}}+R\\_{\\text{correct}}italic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT + italic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT, each described in detail below:\n",
      "\n",
      "#### Format Reward.\n",
      "\n",
      "The format reward ℛformat∈{0,1}subscriptℛformat01\\mathcal{R}\\_{\\text{format}}\\in\\{0,1\\}caligraphic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT ∈ { 0 , 1 } checks whether the model output contains all required special tokens in the correct order as specified by the ground truth:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛformat={1,  if all required fields appear and are in the correct order 0,otherwisesubscriptℛformatcases1  if all required fields appear and are in the correct order 0otherwise\\mathcal{R}\\_{\\text{format}}=\\begin{cases}1,&\\vbox{\\hbox{\\parbox{156.49014pt}{% if all required fields appear\\\\ and are in the correct order}}}\\vspace{8pt}\\\\ 0,&\\text{otherwise}\\end{cases}caligraphic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT = { start\\_ROW start\\_CELL 1 , end\\_CELL start\\_CELL end\\_CELL end\\_ROW start\\_ROW start\\_CELL 0 , end\\_CELL start\\_CELL otherwise end\\_CELL end\\_ROW |  |\n",
      "\n",
      "#### Correctness Reward.\n",
      "\n",
      "The correctness reward ℛcorrect∈[−3,3]subscriptℛcorrect33\\mathcal{R}\\_{\\text{correct}}\\in[-3,3]caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT ∈ [ - 3 , 3 ] evaluates predicted tool calls P={P1,…,Pm}𝑃subscript𝑃1…subscript𝑃𝑚P=\\{P\\_{1},...,P\\_{m}\\}italic\\_P = { italic\\_P start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , … , italic\\_P start\\_POSTSUBSCRIPT italic\\_m end\\_POSTSUBSCRIPT } against ground-truth calls G={G1,…,Gn}𝐺subscript𝐺1…subscript𝐺𝑛G=\\{G\\_{1},...,G\\_{n}\\}italic\\_G = { italic\\_G start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , … , italic\\_G start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT }. It includes three components:\n",
      "\n",
      "* •\n",
      "\n",
      "  Tool Name Matching:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rname=|NG∩NP||NG∪NP|∈[0,1]subscript𝑟namesubscript𝑁𝐺subscript𝑁𝑃subscript𝑁𝐺subscript𝑁𝑃01r\\_{\\text{name}}=\\frac{|N\\_{G}\\cap N\\_{P}|}{|N\\_{G}\\cup N\\_{P}|}\\in[0,1]italic\\_r start\\_POSTSUBSCRIPT name end\\_POSTSUBSCRIPT = divide start\\_ARG | italic\\_N start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ∩ italic\\_N start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT | end\\_ARG start\\_ARG | italic\\_N start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ∪ italic\\_N start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT | end\\_ARG ∈ [ 0 , 1 ] |  |\n",
      "\n",
      "  where NGsubscript𝑁𝐺N\\_{G}italic\\_N start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT and NPsubscript𝑁𝑃N\\_{P}italic\\_N start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT are the sets of tool names extracted from the ground-truth and predicted tool calls, respectively.\n",
      "* •\n",
      "\n",
      "  Parameter Name Matching:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rparam=∑Gj∈G|keys⁢(PG)∩keys⁢(PP)||keys⁢(PG)∪keys⁢(PP)|∈[0,|G|]subscript𝑟paramsubscriptsubscript𝐺𝑗𝐺keyssubscript𝑃𝐺keyssubscript𝑃𝑃keyssubscript𝑃𝐺keyssubscript𝑃𝑃0𝐺r\\_{\\text{param}}=\\sum\\_{G\\_{j}\\in G}\\frac{|\\text{keys}(P\\_{G})\\cap\\text{keys}(P\\_{% P})|}{|\\text{keys}(P\\_{G})\\cup\\text{keys}(P\\_{P})|}\\in[0,|G|]italic\\_r start\\_POSTSUBSCRIPT param end\\_POSTSUBSCRIPT = ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT divide start\\_ARG | keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ) ∩ keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ) | end\\_ARG start\\_ARG | keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ) ∪ keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ) | end\\_ARG ∈ [ 0 , | italic\\_G | ] |  |\n",
      "\n",
      "  where keys⁢(PG)keyssubscript𝑃𝐺\\text{keys}(P\\_{G})keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ) and keys⁢(PP)keyssubscript𝑃𝑃\\text{keys}(P\\_{P})keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ) represent the parameter names of the predicted and ground-truth tool calls, respectively.\n",
      "* •\n",
      "\n",
      "  Parameter Content Matching:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rvalue=∑Gj∈G∑k∈keys⁢(Gj)𝟙⁢[PG⁢[k]=PP⁢[k]]∈[0,∑Gj∈G|keys⁢(Gj)|]subscript𝑟valuesubscriptsubscript𝐺𝑗𝐺subscript𝑘keyssubscript𝐺𝑗1delimited-[]subscript𝑃𝐺delimited-[]𝑘subscript𝑃𝑃delimited-[]𝑘0subscriptsubscript𝐺𝑗𝐺keyssubscript𝐺𝑗\\begin{split}r\\_{\\text{value}}=\\sum\\_{G\\_{j}\\in G}\\ \\sum\\_{k\\in\\text{keys}(G\\_{j})}% \\mathds{1}[P\\_{G}[k]=P\\_{P}[k]]\\\\ \\in[0,\\sum\\_{G\\_{j}\\in G}|\\text{keys}(G\\_{j})|]\\end{split}start\\_ROW start\\_CELL italic\\_r start\\_POSTSUBSCRIPT value end\\_POSTSUBSCRIPT = ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT ∑ start\\_POSTSUBSCRIPT italic\\_k ∈ keys ( italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ) end\\_POSTSUBSCRIPT blackboard\\_1 [ italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT [ italic\\_k ] = italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT [ italic\\_k ] ] end\\_CELL end\\_ROW start\\_ROW start\\_CELL ∈ [ 0 , ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT | keys ( italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ) | ] end\\_CELL end\\_ROW |  |\n",
      "\n",
      "  where PG[k]]P\\_{G}[k]]italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT [ italic\\_k ] ] and PP⁢[k]subscript𝑃𝑃delimited-[]𝑘P\\_{P}[k]italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT [ italic\\_k ] represent the values of the parameters for the predicted and ground truth tool calls.\n",
      "* •\n",
      "\n",
      "  Total match score for each match is:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rmatch=rname+rparam+rvalue∈[0,Smax]subscript𝑟matchsubscript𝑟namesubscript𝑟paramsubscript𝑟value0subscript𝑆r\\_{\\text{match}}=r\\_{\\text{name}}+r\\_{\\text{param}}+r\\_{\\text{value}}\\in[0,S\\_{% \\max}]italic\\_r start\\_POSTSUBSCRIPT match end\\_POSTSUBSCRIPT = italic\\_r start\\_POSTSUBSCRIPT name end\\_POSTSUBSCRIPT + italic\\_r start\\_POSTSUBSCRIPT param end\\_POSTSUBSCRIPT + italic\\_r start\\_POSTSUBSCRIPT value end\\_POSTSUBSCRIPT ∈ [ 0 , italic\\_S start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT ] |  |\n",
      "\n",
      "  where Smax=1+|G|+∑Gj∈G|keys⁢(Gj)|subscript𝑆1𝐺subscriptsubscript𝐺𝑗𝐺keyssubscript𝐺𝑗S\\_{\\max}=1+|G|+\\sum\\_{G\\_{j}\\in G}|\\text{keys}(G\\_{j})|italic\\_S start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT = 1 + | italic\\_G | + ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT | keys ( italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ) | denotes the maximum possible score.\n",
      "\n",
      "The total score is computed by finding the optimal matching between P𝑃Pitalic\\_P and G𝐺Gitalic\\_G to maximize the total match score:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛcorrect=6⋅RmaxSmax−3∈[−3,3]subscriptℛcorrect⋅6subscript𝑅subscript𝑆333\\mathcal{R}\\_{\\text{correct}}=6\\cdot\\frac{R\\_{\\max}}{S\\_{\\max}}-3\\in[-3,3]caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT = 6 ⋅ divide start\\_ARG italic\\_R start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_S start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT end\\_ARG - 3 ∈ [ - 3 , 3 ] |  |\n",
      "\n",
      "where Rmaxsubscript𝑅R\\_{\\max}italic\\_R start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT denotes the total match score from the optimal matching. The final correctness reward ℛcorrectsubscriptℛcorrect\\mathcal{R}\\_{\\text{correct}}caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT is the normalized reward for the matching process. We empirically set the reward scale within the range of [−3,3]33[-3,3][ - 3 , 3 ], with more analysis and ablatiions of reward scale presented in [Section 5](https://arxiv.org/html/2504.13958v1#S5 \"5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\").\n",
      "\n",
      "The final reward value ℛfinalsubscriptℛfinal\\mathcal{R}\\_{\\text{final}}caligraphic\\_R start\\_POSTSUBSCRIPT final end\\_POSTSUBSCRIPT is finally derived as the sum of ℛformatsubscriptℛformat\\mathcal{R}\\_{\\text{format}}caligraphic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT and ℛcorrectsubscriptℛcorrect\\mathcal{R}\\_{\\text{correct}}caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛfinal=ℛformat+ℛcorrect∈[−3,4]subscriptℛfinalsubscriptℛformatsubscriptℛcorrect34\\mathcal{R}\\_{\\text{final}}=\\mathcal{R}\\_{\\text{format}}+\\mathcal{R}\\_{\\text{% correct}}\\in[-3,4]caligraphic\\_R start\\_POSTSUBSCRIPT final end\\_POSTSUBSCRIPT = caligraphic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT + caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT ∈ [ - 3 , 4 ] |  |\n",
      "\n",
      "Unlike prior works that often rely on binary or overly simplified reward signals, our design captures the nuanced structure of tool calls by evaluating multiple interdependent components including tool names, parameter schemas, and parameter values. This fine-grained formulation better reflects the complexity of real-world tool use, where correctness cannot be reduced to a single binary criterion. We further validate the impact of this design through comprehensive analysis in [Section 5](https://arxiv.org/html/2504.13958v1#S5 \"5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\").\n",
      "\n",
      "Overall, our reward design ensures a balanced and interpretable evaluation signal by explicitly separating structural compliance from semantic correctness. By aligning rewards with both format adherence and fine-grained tool call accuracy, the model is guided to produce outputs that are not only syntactically valid but also semantically faithful, which is crucial for downstream tool execution and final task success.\n",
      "\n",
      "### 3.4 RL Training with GRPO\n",
      "\n",
      "To tune the model with structured rewards, we employ GRPO, a variant of PPO that introduces advantage normalization within grouped samples. This normalization helps stabilize training by reducing variance across samples that share a common input context. Let πθsubscript𝜋𝜃\\pi\\_{\\theta}italic\\_π start\\_POSTSUBSCRIPT italic\\_θ end\\_POSTSUBSCRIPT represent the current policy.\n",
      "\n",
      "#### Normalized Advantage Across Query Groups.\n",
      "\n",
      "For each query Q𝑄Qitalic\\_Q, its responses derived from the rollout form a group GQsubscript𝐺𝑄G\\_{Q}italic\\_G start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT consisting of multiple responses and their corresponding reward values:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | GQ={A,(s1,r1),(s2,r2),…,(sn,rn)}subscript𝐺𝑄𝐴subscript𝑠1subscript𝑟1subscript𝑠2subscript𝑟2…subscript𝑠𝑛subscript𝑟𝑛G\\_{Q}=\\{A,(s\\_{1},r\\_{1}),(s\\_{2},r\\_{2}),\\ldots,(s\\_{n},r\\_{n})\\}italic\\_G start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT = { italic\\_A , ( italic\\_s start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT , italic\\_r start\\_POSTSUBSCRIPT 1 end\\_POSTSUBSCRIPT ) , ( italic\\_s start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT , italic\\_r start\\_POSTSUBSCRIPT 2 end\\_POSTSUBSCRIPT ) , … , ( italic\\_s start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT , italic\\_r start\\_POSTSUBSCRIPT italic\\_n end\\_POSTSUBSCRIPT ) } |  |\n",
      "\n",
      "where A𝐴Aitalic\\_A denotes the ground-truth annotation for Q𝑄Qitalic\\_Q, and each reward risubscript𝑟𝑖r\\_{i}italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT is computed as the sum of the format and correctness rewards associated with response sisubscript𝑠𝑖s\\_{i}italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT, i.e., ri=ℛformat⁢(si,A)+ℛcorrect⁢(si,A)subscript𝑟𝑖subscriptℛformatsubscript𝑠𝑖𝐴subscriptℛcorrectsubscript𝑠𝑖𝐴r\\_{i}=\\mathcal{R}\\_{\\text{format}}(s\\_{i},A)+\\mathcal{R}\\_{\\text{correct}}(s\\_{i},A)italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT = caligraphic\\_R start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , italic\\_A ) + caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , italic\\_A ). For each group, we calculate the mean and standard deviation of the rewards:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | μQ=1n⁢∑i=1nri,σQ=1n⁢∑i=1n(ri−μQ)2formulae-sequencesubscript𝜇𝑄1𝑛superscriptsubscript𝑖1𝑛subscript𝑟𝑖subscript𝜎𝑄1𝑛superscriptsubscript𝑖1𝑛superscriptsubscript𝑟𝑖subscript𝜇𝑄2\\mu\\_{Q}=\\frac{1}{n}\\sum\\_{i=1}^{n}r\\_{i},\\quad\\sigma\\_{Q}=\\sqrt{\\frac{1}{n}\\sum\\_{% i=1}^{n}(r\\_{i}-\\mu\\_{Q})^{2}}italic\\_μ start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT = divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_n end\\_ARG ∑ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_n end\\_POSTSUPERSCRIPT italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT , italic\\_σ start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT = square-root start\\_ARG divide start\\_ARG 1 end\\_ARG start\\_ARG italic\\_n end\\_ARG ∑ start\\_POSTSUBSCRIPT italic\\_i = 1 end\\_POSTSUBSCRIPT start\\_POSTSUPERSCRIPT italic\\_n end\\_POSTSUPERSCRIPT ( italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT - italic\\_μ start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT ) start\\_POSTSUPERSCRIPT 2 end\\_POSTSUPERSCRIPT end\\_ARG |  |\n",
      "\n",
      "Then, for each sample sisubscript𝑠𝑖s\\_{i}italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT in the group, we define the normalized advantage:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | Ai⁢(si|Q)=ri−μQσQ+ηsubscript𝐴𝑖conditionalsubscript𝑠𝑖𝑄subscript𝑟𝑖subscript𝜇𝑄subscript𝜎𝑄𝜂A\\_{i}(s\\_{i}|Q)=\\frac{r\\_{i}-\\mu\\_{Q}}{\\sigma\\_{Q}+\\eta}italic\\_A start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) = divide start\\_ARG italic\\_r start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT - italic\\_μ start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_σ start\\_POSTSUBSCRIPT italic\\_Q end\\_POSTSUBSCRIPT + italic\\_η end\\_ARG |  |\n",
      "\n",
      "where η𝜂\\etaitalic\\_η is a constant to avoid division by zero.\n",
      "\n",
      "#### Policy Optimization Objective.\n",
      "\n",
      "The policy πθsubscript𝜋𝜃\\pi\\_{\\theta}italic\\_π start\\_POSTSUBSCRIPT italic\\_θ end\\_POSTSUBSCRIPT is optimized using the standard clipped PPO objective, adapted with our group-wise normalized advantages:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | JGRPO(θ)=𝔼Q∼𝒟𝔼si∼πθ[min(πθ⁢(si|Q)πold⁢(si|Q)Ai(si|Q),clip(πθ⁢(si|Q)πold⁢(si|Q),1−ϵ,1+ϵ)Ai(si|Q))]subscript𝐽GRPO𝜃subscript𝔼similar-to𝑄𝒟subscript𝔼similar-tosubscript𝑠𝑖subscript𝜋𝜃delimited-[]subscript𝜋𝜃conditionalsubscript𝑠𝑖𝑄subscript𝜋oldconditionalsubscript𝑠𝑖𝑄subscript𝐴𝑖|subscript𝑠𝑖𝑄clipsubscript𝜋𝜃conditionalsubscript𝑠𝑖𝑄subscript𝜋oldconditionalsubscript𝑠𝑖𝑄1italic-ϵ1italic-ϵsubscript𝐴𝑖|subscript𝑠𝑖𝑄\\begin{split}J\\_{\\text{GRPO}}(\\theta)=\\mathbb{E}\\_{Q\\sim\\mathcal{D}}\\mathbb{E}\\_{% s\\_{i}\\sim\\pi\\_{\\theta}}\\Big{[}\\min\\Big{(}\\frac{\\pi\\_{\\theta}(s\\_{i}|Q)}{\\pi\\_{% \\text{old}}(s\\_{i}|Q)}A\\_{i}(s\\_{i}|Q),\\\\ \\text{clip}\\big{(}\\frac{\\pi\\_{\\theta}(s\\_{i}|Q)}{\\pi\\_{\\text{old}}(s\\_{i}|Q)},1-% \\epsilon,1+\\epsilon\\big{)}A\\_{i}(s\\_{i}|Q)\\Big{)}\\Big{]}\\end{split}start\\_ROW start\\_CELL italic\\_J start\\_POSTSUBSCRIPT GRPO end\\_POSTSUBSCRIPT ( italic\\_θ ) = blackboard\\_E start\\_POSTSUBSCRIPT italic\\_Q ∼ caligraphic\\_D end\\_POSTSUBSCRIPT blackboard\\_E start\\_POSTSUBSCRIPT italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ∼ italic\\_π start\\_POSTSUBSCRIPT italic\\_θ end\\_POSTSUBSCRIPT end\\_POSTSUBSCRIPT [ roman\\_min ( divide start\\_ARG italic\\_π start\\_POSTSUBSCRIPT italic\\_θ end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) end\\_ARG start\\_ARG italic\\_π start\\_POSTSUBSCRIPT old end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) end\\_ARG italic\\_A start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) , end\\_CELL end\\_ROW start\\_ROW start\\_CELL clip ( divide start\\_ARG italic\\_π start\\_POSTSUBSCRIPT italic\\_θ end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) end\\_ARG start\\_ARG italic\\_π start\\_POSTSUBSCRIPT old end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) end\\_ARG , 1 - italic\\_ϵ , 1 + italic\\_ϵ ) italic\\_A start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT ( italic\\_s start\\_POSTSUBSCRIPT italic\\_i end\\_POSTSUBSCRIPT | italic\\_Q ) ) ] end\\_CELL end\\_ROW |  |\n",
      "\n",
      "Unlike the original GRPO formulations, we omit the KL penalty term against a reference model. This design choice encourages the model to more freely adapt its behavior to our custom response format and structured reward signals. In practice, we observe that this leads to faster convergence and comparable performance, while also simplifying the training pipeline.\n",
      "\n",
      "Overall, this objective guides the policy to generate structurally consistent and semantically accurate tool calls, while group-wise normalization mitigates reward variance across queries, leading to more stable and sample-efficient alignment with task-specific response requirements.\n",
      "\n",
      "4 Experiments\n",
      "-------------\n",
      "\n",
      "### 4.1 Training Dataset\n",
      "\n",
      "To support robust tool learning through RL, we construct a mixed dataset spanning diverse tool use scenarios:\n",
      "\n",
      "* •\n",
      "\n",
      "  ToolACE (Liu et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib25)): A general tool use dataset where the model learns when to invoke tools versus respond directly, improving decision-making in multi-step interactions.\n",
      "* •\n",
      "\n",
      "  Hammer (Masked) (Lin et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib23)): A subset of Hammer with randomized tool and parameter names, forcing the model to rely on descriptions rather than memorized labels, thus enhancing generalization and reducing overfitting to certain tools.\n",
      "* •\n",
      "\n",
      "  xLAM (Zhang et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib59)): A compositional dataset requiring one or multiple tool calls per turn, encouraging the model to reason about tool dependencies and plan diverse tool calling action actively.\n",
      "\n",
      "For RL training, we sample 2K examples from ToolACE and 1K each from Hammer and xLAM, creating a balanced dataset spanning diverse levels of complexity and tool use. Multi-step trajectories are decomposed into single-step instances, with prior dialogue history injected into the user prompt (as shown in [Figure 11](https://arxiv.org/html/2504.13958v1#Ax1.F11 \"In Appendix ‣ ToolRL: Reward is All Tool Learning Needs\")) to preserve context. This setup encourages strategic exploration and teaches the model to select and apply tools appropriately within each step. Please see [Appendix B](https://arxiv.org/html/2504.13958v1#A2 \"Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\") for more details and justifications.\n",
      "\n",
      "### 4.2 Experiment Settings\n",
      "\n",
      "#### Training.\n",
      "\n",
      "We conduct all RL experiments using the veRL framework (Sheng et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib44)), adopting the GRPO algorithm detailed in the previous section. For each training step, we sample a batch of 512, and generate 4 responses per query, training for 15 epochs in total (see [Appendix B](https://arxiv.org/html/2504.13958v1#A2 \"Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\") for full configuration details). To encourage broader policy exploration, we remove KL regularization and apply a generation temperature of 1.0. We initialize our models with the Qwen-2.5-Instruct (Team, [2024](https://arxiv.org/html/2504.13958v1#bib.bib47)) and Llama-3.2-Instruct (Dubey et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib9)) series, which are further tuned under the GRPO objective with our customized reward design.\n",
      "\n",
      "#### Evaluation.\n",
      "\n",
      "We evaluate our approach on the Berkeley Function Call Leaderboard (BFCL) (Patil et al., [2024](https://arxiv.org/html/2504.13958v1#bib.bib28)), a comprehensive benchmark that spans a diverse set of challenges, including single-step reasoning, multi-step tool use, real-time execution, irrelevant tool rejection, simultaneous multi-tool selection, and multi-tool application222<https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html>. In addition, we present results on API-Bank (Li et al., [2023](https://arxiv.org/html/2504.13958v1#bib.bib19)), a three-level evaluation framework comprising 73 diverse and complex API tools. It assesses an LLM’s ability to select and apply tools through natural multi-turn dialogues, across three levels of difficulty. We also evaluate on a representative QA benchmark Bamboogle (Press et al., [2022](https://arxiv.org/html/2504.13958v1#bib.bib29)), which comprises a variety of question-answering tasks where performance is measured based on the final answer accuracy rather than the correctness of tool use. These broad coverage makes our evaluation setting effective for evaluating real-world LLM tool use proficiency. All results are reported in terms of accuracy.\n",
      "\n",
      "#### Baselines.\n",
      "\n",
      "We compare our approach against several baselines to better isolate the effects of GRPO training: (1) Raw Instruct Model: the original model without any additional fine-tuning or RL, evaluated using the same prompts. (2) SFT on RL Data: the instruct model fine-tuned using the same 4K / selected 400 data points as the RL training set, providing a comparison point to assess whether GRPO training outperforms standard SFT. (3) GRPO on SFT Model: GRPO is applied to a model that has already undergone SFT on the selected 400 data points. This setup allows us to evaluate the impact of initializing GRPO with a format-aware model, in contrast to starting from the raw instruct model in a cold start manner. (4) PPO: We also include the standard PPO setting as a baseline to evaluate whether our reward design is effective beyond GRPO. We report results for both a cold start PPO model and a PPO model initialized with SFT, using the same hyperparameters as in the GRPO setup for a fair comparison. Please refer to [Appendix B](https://arxiv.org/html/2504.13958v1#A2 \"Appendix B Experiment Details ‣ ToolRL: Reward is All Tool Learning Needs\") for more details and justifications.\n",
      "\n",
      "### 4.3 Results\n",
      "\n",
      "|  |  |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Non-Live AST Acc | Non-Live Exec Acc | Live Acc | Multi Turn Acc | Relevance Detection | Irrelevance Detection |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 19.41% | 16.00% | 13.18% | 35.58% | 0.00% | 44.44% | 82.49% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400) | 40.21% | 65.12% | 61.11% | 56.69% | 1.00% | 94.44% | 60.14% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k) | 40.67% | 59.94% | 59.84% | 59.31% | 1.00% | 88.89% | 71.34% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 42.95% | 77.65% | 69.75% | 55.73% | 1.88% | 100.00% | 48.40% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 40.93% | 70.54% | 60.79% | 56.33% | 1.00% | 94.44% | 58.63% |\n",
      "| Qwen2.5-1.5B-Instruct (PPO Cold Start) | 38.32% | 79.40% | 70.11% | 45.24% | 0.87% | 100.00% | 18.09% |\n",
      "| Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) | 46.20% | 77.96% | 76.98% | 60.73% | 2.25% | 100.00% | 56.44% |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 33.04% | 42.52% | 40.80% | 53.96% | 1.00% | 64.71% | 56.01% |\n",
      "| Qwen2.5-3B-Instruct (SFT400) | 34.08% | 69.29% | 61.50% | 41.40% | 0.00% | 94.44% | 8.11% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k) | 41.97% | 62.85% | 54.73% | 59.17% | 0.75% | 77.78% | 75.12% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 45.80% | 78.29% | 71.09% | 58.76% | 5.12% | 94.12% | 54.70% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 46.42% | 76.21% | 68.93% | 64.15% | 1.75% | 88.89% | 71.76% |\n",
      "| Qwen2.5-3B-Instruct (PPO Cold Start) | 51.15% | 82.42% | 78.52% | 67.78% | 4.88% | 94.12% | 73.87% |\n",
      "| Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) | 52.98% | 81.58% | 79.43% | 73.78% | 3.75% | 88.24% | 84.85% |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 41.97% | 66.02% | 70.11% | 53.51% | 4.25% | 76.47% | 62.66% |\n",
      "| Qwen2.5-7B-Instruct (SFT400) | 34.08% | 69.29% | 66.68% | 41.4% | 0.00% | 94.44% | 8.11% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k) | 36.53% | 45.15% | 53.5% | 57.13% | 0.75% | 72.22% | 72.32% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 42.02% | 83.90% | 72.62% | 51.84% | 0.25% | 100.00% | 29.66% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 39.25% | 80.69% | 74.34% | 46.51% | 0.25% | 100.00% | 14.19% |\n",
      "| Qwen2.5-7B-Instruct (PPO Cold Start) | 46.68% | 79.33% | 78.16% | 63.17% | 0.38% | 88.89% | 52.92% |\n",
      "| Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) | 58.38% | 86.17% | 78.25% | 74.9% | 18.12% | 83.33% | 76.68% |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 22.09% | 17.44% | 14.57% | 43.85% | 0.00% | 77.78% | 66.07% |\n",
      "| Llama-3.2-3B-Instruct (SFT400) | 41.22% | 64.27% | 62.18% | 58.37% | 0.75% | 66.67% | 71.12% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k) | 44.16% | 65.42% | 67.02% | 63.04% | 1.38% | 77.78% | 78.25% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 41.62% | 68.10% | 69.88% | 52.98% | 3.00% | 94.12% | 56.29% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 42.54% | 65.15% | 68.98% | 59.40% | 0.88% | 72.22% | 65.80% |\n",
      "| Llama-3.2-3B-Instruct (PPO Cold Start) | 42.98% | 84.00% | 72.00% | 52.80% | 2.88% | 100.00% | 31.94% |\n",
      "| Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) | 44.10% | 74.38% | 75.18% | 56.86% | 1.37% | 94.44% | 62.23% |\n",
      "\n",
      "Table 1: BFCL V3 Benchmark Results (Main Result)\n",
      "\n",
      "\n",
      "\n",
      "|  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Level 1 | Level 2 | Level 3 |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 30.65% | 28.32% | 35.82% | 35.11% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400) | 53.60% | 57.14% | 50.75% | 44.27% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k) | 47.07% | 52.88% | 52.24% | 26.72% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 57.12% | 60.9% | 50.75% | 48.85% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 61.31% | 64.16% | 58.21% | 54.20% |\n",
      "| Qwen2.5-1.5B-Instruct (PPO Cold Start) | 40.54% | 44.61% | 31.34% | 32.82% |\n",
      "| Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) | 63.15% | 70.68% | 61.19% | 41.22% |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 51.59% | 59.65% | 32.84% | 36.64% |\n",
      "| Qwen2.5-3B-Instruct (SFT400) | 52.76% | 59.65% | 50.75% | 32.82% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k) | 50.92% | 55.64% | 43.28% | 40.46% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 65.16% | 67.92% | 55.22% | 61.83% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 62.48% | 68.67% | 58.21% | 45.80% |\n",
      "| Qwen2.5-3B-Instruct (PPO Cold Start) | 57.62% | 64.66% | 59.70% | 35.11% |\n",
      "| Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) | 67.00% | 73.43% | 67.16% | 47.33% |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 62.48% | 70.68% | 49.25% | 44.27% |\n",
      "| Qwen2.5-7B-Instruct (SFT400) | 50.59% | 55.89% | 50.75% | 34.35% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k) | 47.07% | 51.13% | 34.33% | 41.22% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 63.15% | 72.43% | 58.21% | 37.40% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 54.10% | 61.40% | 52.24% | 32.82% |\n",
      "| Qwen2.5-7B-Instruct (PPO Cold Start) | 61.64% | 68.67% | 44.78% | 48.85% |\n",
      "| Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) | 64.66% | 73.93% | 61.19% | 38.17% |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 40.54% | 44.86% | 29.85% | 32.82% |\n",
      "| Llama-3.2-3B-Instruct (SFT400) | 52.76% | 60.65% | 35.82% | 37.40% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k) | 43.89% | 53.88% | 29.85% | 20.61% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 57.79% | 63.16% | 47.76% | 46.56% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 56.78% | 63.60% | 41.79% | 43.51% |\n",
      "| Llama-3.2-3B-Instruct (PPO Cold Start) | 55.78% | 60.65% | 41.79% | 48.09% |\n",
      "| Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) | 59.13% | 65.66% | 52.24% | 42.75% |\n",
      "\n",
      "Table 2: API-Bank Test Results (Main Result)\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "| Model | Accuracy | Avg Num Tool Call |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 20.8% | 0.61 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400) | 24.8% | 0.78 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k) | 23.2% | 1.25 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 36.8% | 1.06 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 38.4% | 0.96 |\n",
      "| Qwen2.5-1.5B-Instruct (PPO Cold Start) | 23.2% | 2.38 |\n",
      "| Qwen2.5-1.5B-Instruct (Ours, GRPO Cold Start) | 44.0% | 1.19 |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 52.0% | 1.77 |\n",
      "| Qwen2.5-3B-Instruct (SFT400) | 54.4% | 0.86 |\n",
      "| Qwen2.5-3B-Instruct (SFT4k) | 49.6% | 0.92 |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 43.2% | 1.04 |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 56.8% | 0.99 |\n",
      "| Qwen2.5-3B-Instruct (PPO Cold Start) | 40.0% | 1.14 |\n",
      "| Qwen2.5-3B-Instruct (Ours, GRPO Cold Start) | 60.0% | 1.32 |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 69.6% | 1.42 |\n",
      "| Qwen2.5-7B-Instruct (SFT400) | 28.8% | 3.71 |\n",
      "| Qwen2.5-7B-Instruct (SFT4k) | 30.4% | 1.06 |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 45.6% | 3.54 |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 29.6% | 3.70 |\n",
      "| Qwen2.5-7B-Instruct (PPO Cold Start) | 48.0% | 1.25 |\n",
      "| Qwen2.5-7B-Instruct (Ours, GRPO Cold Start) | 72.0% | 1.63 |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 34.4% | 1.25 |\n",
      "| Llama-3.2-3B-Instruct (SFT400) | 44.0% | 0.98 |\n",
      "| Llama-3.2-3B-Instruct (SFT4k) | 48.8% | 0.98 |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 39.2% | 1.33 |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 45.6% | 1.00 |\n",
      "| Llama-3.2-3B-Instruct (PPO Cold Start) | 29.6% | 1.42 |\n",
      "| Llama-3.2-3B-Instruct (Ours, GRPO Cold Start) | 52.0% | 0.89 |\n",
      "\n",
      "Table 3: Bamboogle Test Results (Main Result)\n",
      "\n",
      "#### Main Results.\n",
      "\n",
      "We report BFCL and API-Bank results in [Table 1](https://arxiv.org/html/2504.13958v1#S4.T1 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\") and [Table 3](https://arxiv.org/html/2504.13958v1#S4.T3 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\"), respectively. Our GRPO method, trained from scratch on the Qwen2.5-Instruct series, generally outperforms other baselines, achieving 1̃0% absolute gains over SFT trained on the same data volume. In contrast, LLaMA-3.2-Instruct shows less improvement, possibly due to the model’s lower adaptability to GRPO-style generalization. Nevertheless, it remains competitive and outperforms most baselines on API-Bank.\n",
      "\n",
      "#### SFT Initialization Impacts.\n",
      "\n",
      "Interestingly, GRPO also improves models initialized with limited SFT, often outperforming full-scale SFT trained on 10 times more data. However, this setup still underperforms compared to cold start GRPO. We hypothesize that SFT initialization leads to memorization and overfitting, which reduces the impact of GRPO’s effectiveness in generalization. As shown in LABEL:fig:main\\_initialization, SFT-initialized models achieve higher training rewards due to distributional alignment between SFT and RL data, but empirically generalize worse on the two benchmarks. This further highlights that higher training rewards do not necessarily translate to better generalization.\n",
      "\n",
      "#### Reward Design on PPO.\n",
      "\n",
      "We also evaluate PPO under both cold start and SFT-initialized settings to examine the effectiveness of our reward design. The results show that while PPO with a cold start can outperform SFT in some cases, it tends to be less stable across different model settings. In contrast, GRPO consistently achieves higher rewards even from a cold start, suggesting that our reward design is partially effective for PPO but works best in the GRPO framework. As shown in LABEL:fig:main\\_ppo, GRPO not only achieves higher correctness rewards but also gains format rewards more rapidly during training. Interestingly, PPO benefits from SFT initialization, generally yielding better results than a cold start, whereas GRPO performs better when trained from scratch. These findings highlight that while PPO can benefit from our reward design, its impact is more limited compared to the more robust and consistent improvements observed with GRPO.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|  |\n",
      "| --- |\n",
      "| User: I would like to buy a movie ticket in San Jose at 11 o’clock in the night. |\n",
      "| Tools Available: |\n",
      "| 1. Movies\\_1\\_BuyMovieTickets(movie\\_name, number\\_of\\_tickets, ...) |\n",
      "| → Buy tickets for a movie at a specific time and location. |\n",
      "| 2. Movies\\_1\\_FindMovies(location, ...) |\n",
      "| → Search movies by location, genre, or format. |\n",
      "| 3. Movies\\_1\\_GetTimesForMovie(movie\\_name, location, ...) |\n",
      "| → Get showtimes for a movie on a specific date and location. |\n",
      "| <think> The user wants to buy a movie ticket in San Jose at 11 PM. The available |\n",
      "| tool, Movies\\_1\\_BuyMovieTickets, allows us to purchase movie tickets with the |\n",
      "| required details such as movie name, number of tickets, show date, location, show time, |\n",
      "| and show type. We need to find the movie name and the show date. However, the user |\n",
      "| did not specify the movie name or the show date. We can infer the show date from the |\n",
      "| current date, but we need to know the movie name to proceed with purchase. </think> |\n",
      "| <response> To assist in purchasing a movie ticket, I need the name of the movie and |\n",
      "| the specific date of the show. Could you please provide this information? </response> |\n",
      "\n",
      "|  |\n",
      "| --- |\n",
      "| User: What is the slope of the line which is perpendicular to the line with the |\n",
      "| equation y = 3x + 2? |\n",
      "| Tools Available: |\n",
      "| 1. find\\_critical\\_points(function, variable, range=[0.0, 3.4]) |\n",
      "| → Find critical points of a function with respect to a variable, optionally |\n",
      "| within a range. |\n",
      "| <think> The problem is asking for the slope of a line that is perpendicular |\n",
      "| to the line with the equation y = 3x + 2. The slope of the given line is 3. The slope |\n",
      "| of a line that is perpendicular to another line is the negative reciprocal of the |\n",
      "| original line’s slope. So, the slope of the line perpendicular to y = 3x + 2 would be -1/3. |\n",
      "| The given tool find\\_critical\\_points is not relevant to finding the slope of a line and |\n",
      "| cannot be used to solve this problem. </think> |\n",
      "| <response> The slope of the line that is perpendicular to the line with the |\n",
      "| equation y = 3x + 2 is -1/3. </response> |\n",
      "\n",
      "Table 4: Cases of Qwen2.5-3B-Instruct (GRPO Cold Start) generalizing to irrelevant tool detection goals.\n",
      "\n",
      "#### Generalization Studies.\n",
      "\n",
      "We evaluate the generalization ability of our trained model in two challenging settings: unfamiliar scenarios and novel task goals (both from BFCL benchmark subset). Specifically, we test the model’s performance in tool usage within unseen programming languages and its ability to detect irrelevant tools, neither of which were explicitly included during RL training or in the dataset. As shown in LABEL:fig:main\\_generalization, Qwen2.5-3B-Instruct, when trained from scratch with our GRPO-based reward design, consistently achieves highest performance. Additionally, [Table 4](https://arxiv.org/html/2504.13958v1#S4.T4 \"In Reward Design on PPO. ‣ 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\") presents two qualitative examples where the model proactively rejects inappropriate tool use—first by clarifying ambiguous intent, and second by opting to answer directly without tools. These behaviors reflect emergent proactivity and metacognition, enhancing efficiency, reducing hallucinations, and signaling foundational agentic intelligence.\n",
      "\n",
      "#### Free-form Inference Effectiveness.\n",
      "\n",
      "While our model is trained with a focus on tool call format and correctness, we further evaluate its ability to handle free-form tool use in a QA setting. Unlike the structured tool selection and application tasks, QA setting: (1) imposes no constraints on tool call parameters, and (2) evaluates only the final answer, making it a “goal-oriented” rather than a “process-oriented” task. This naturally introduces a multi-step interaction scenario.\n",
      "\n",
      "Specifically, we use Bamboogle, a multi-hop QA dataset, to assess this capability. The model is equipped with a web search tool, and we report both the answer accuracy and the number of tool calls for all baselines and our approach. As shown in [Table 3](https://arxiv.org/html/2504.13958v1#S4.T3 \"In 4.3 Results ‣ 4 Experiments ‣ ToolRL: Reward is All Tool Learning Needs\"), our reward design achieves the highest performance, despite this setting not being explicitly seen during training. Notably, our cold start GRPO model surpasses others in accuracy without relying on excessive number of tool calls. This suggests that the model can flexibly invoke tools when needed, effectively leverage feedback, wisely and efficiently navigating toward the correct answer.\n",
      "\n",
      "5 Analysis\n",
      "----------\n",
      "\n",
      "In this section, we conduct a series of ablation studies to identify the most effective reward design for tool calling. We explore various factors including reward type, scale, granularity, and temporal dynamics.\n",
      "\n",
      "### 5.1 Effect of Length Reward\n",
      "\n",
      "We first examine the role of a length-based reward. Prior work has demonstrated that the R1-like models can promote deeper reasoning, often reflected in longer thinking traces. To encourage this behavior, we introduce a reward term proportional to the length of the <think> field:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛlength=min⁡(LthinkLtarget,1)subscriptℛlengthsubscript𝐿thinksubscript𝐿target1\\mathcal{R}\\_{\\text{length}}=\\min\\left(\\frac{L\\_{\\text{think}}}{L\\_{\\text{target}% }},1\\right)caligraphic\\_R start\\_POSTSUBSCRIPT length end\\_POSTSUBSCRIPT = roman\\_min ( divide start\\_ARG italic\\_L start\\_POSTSUBSCRIPT think end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_L start\\_POSTSUBSCRIPT target end\\_POSTSUBSCRIPT end\\_ARG , 1 ) |  |\n",
      "\n",
      "where Lthinksubscript𝐿thinkL\\_{\\text{think}}italic\\_L start\\_POSTSUBSCRIPT think end\\_POSTSUBSCRIPT denotes the length of the thinking segment in model’s output, and Ltargetsubscript𝐿targetL\\_{\\text{target}}italic\\_L start\\_POSTSUBSCRIPT target end\\_POSTSUBSCRIPT denotes the target output length, which we empirically set to 512. We found that the raw model rarely generates responses longer than half this length, making 512 a reasonable and effective target for encouraging longer outputs. This length-based component is added to the overall reward, which now consists of format, correctness, and reasoning length.\n",
      "\n",
      "|  |  |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Non-Live AST Acc | Non-Live Exec Acc | Live Acc | Multi Turn Acc | Relevance Detection | Irrelevance Detection |\n",
      "| Qwen2.5-1.5B-Instruct (Original) | 46.20% | 77.96% | 76.98% | 60.73% | 2.25% | 100.00% | 56.44% |\n",
      "| Qwen2.5-1.5B-Instruct (w/ Length Reward) | 33.23% | 70.58% | 71.36% | 35.63% | 0.50% | 94.44% | 4.52% |\n",
      "| Qwen2.5-1.5B-Instruct (Dynamic) | 28.51% | 53.23% | 48.23% | 38.07% | 0.00% | 55.56% | 25.08% |\n",
      "| Qwen2.5-3B-Instruct (Original) | 52.98% | 81.58% | 79.43% | 73.78% | 3.75% | 88.24% | 84.85% |\n",
      "| Qwen2.5-3B-Instruct (w/ Length reward) | 48.89% | 77.83% | 78.61% | 63.56% | 4.50% | 88.24% | 71.22% |\n",
      "| Qwen2.5-3B-Instruct (Dynamic) | 48.24% | 77.60% | 79.11% | 63.22% | 3.00% | 88.89% | 68.53% |\n",
      "| Llama-3.2-3B-Instruct (Original) | 44.10% | 74.38% | 75.18% | 56.86% | 1.37% | 94.44% | 62.23% |\n",
      "| Llama-3.2-3B-Instruct (w/ Length reward) | 44.98% | 78.02% | 77.54% | 56.55% | 1.25% | 100.00% | 63.76% |\n",
      "| Llama-3.2-3B-Instruct (Dynamic) | 43.15% | 75.50% | 71.64% | 56.06% | 1.00% | 100.00% | 57.82% |\n",
      "\n",
      "Table 5: BFCL V3 Benchmark Results (Length)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As shown in LABEL:fig:analysis\\_length, both response length and the length reward generally increase throughout training, particularly for the Qwen model series. This indicates that the length reward effectively encourages longer reasoning. However, the downstream results in [Table 5](https://arxiv.org/html/2504.13958v1#S5.T5 \"In 5.1 Effect of Length Reward ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\") reveal that adding a length reward does not consistently improve task performance, and in smaller-scale models, it can even cause substantial degradation. These observations suggest that while extended reasoning may appear desirable, it is not always beneficial for tool use tasks. In fact, excessive length may introduce unnecessary complexity, leading to overthinking and reduced effectiveness.\n",
      "\n",
      "#### Dynamic Length Reward.\n",
      "\n",
      "Since fixed-length rewards showed minimal impact and converged quickly, we explored a dynamic length reward that adapts over training steps. Specifically, we define:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛdynamic=min⁡(LthinkLtarget⋅(1+p),1)subscriptℛdynamicsubscript𝐿think⋅subscript𝐿target1𝑝1\\mathcal{R}\\_{\\text{dynamic}}=\\min\\left(\\frac{L\\_{\\text{think}}}{L\\_{\\text{target% }}\\cdot(1+p)},1\\right)caligraphic\\_R start\\_POSTSUBSCRIPT dynamic end\\_POSTSUBSCRIPT = roman\\_min ( divide start\\_ARG italic\\_L start\\_POSTSUBSCRIPT think end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_L start\\_POSTSUBSCRIPT target end\\_POSTSUBSCRIPT ⋅ ( 1 + italic\\_p ) end\\_ARG , 1 ) |  |\n",
      "\n",
      "where S𝑆Sitalic\\_S denotes the training steps and p=ScurrentStotal∈[0,1]𝑝subscript𝑆currentsubscript𝑆total01p=\\frac{S\\_{\\text{current}}}{S\\_{\\text{total}}}\\in[0,1]italic\\_p = divide start\\_ARG italic\\_S start\\_POSTSUBSCRIPT current end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_S start\\_POSTSUBSCRIPT total end\\_POSTSUBSCRIPT end\\_ARG ∈ [ 0 , 1 ] represents the normalized training progress. This formulation gradually increases the target thinking length over time, aligning with model maturity.\n",
      "\n",
      "\n",
      "\n",
      "As shown in LABEL:fig:analysis\\_length\\_dynamic, this approach yields a steadier growth in thinking length, particularly for the Llama model. However, the performance results in [Table 5](https://arxiv.org/html/2504.13958v1#S5.T5 \"In 5.1 Effect of Length Reward ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\") reveal that even scheduled rewards fail to improve performance. This further supports our hypothesis that extended reasoning may not benefit this task and can even have adverse effects.\n",
      "\n",
      "{bclogo}\n",
      "\n",
      "[couleur=msftBlack!05, epBord=1, arrondi=0.2, logo=\\bcetoile, marge=5, ombre=true, blur, couleurBord=msftBlack!10, tailleOndu=1, sousTitre=Takeaway 1: While length rewards encourage longer reasoning traces, they do not consistently improve task performance and may even harm it in smaller models, highlighting that longer reasoning is not inherently better for tool use tasks.]\n",
      "\n",
      "### 5.2 Effect of Reward Scale\n",
      "\n",
      "Next, we investigate the effect of reward scaling, specifically the relative weighting between correctness and format rewards. Prior work in R1-style RL commonly assigns a higher weight to correctness reward than to format reward (Xie et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib50); Jin et al., [2025](https://arxiv.org/html/2504.13958v1#bib.bib15)), emphasizing the importance of learning correct answer over superficial adherence to format. This strategy helps prevent reward hacking, where a model might exploit formatting heuristics without learning task semantics.\n",
      "\n",
      "To test the importance of this design choice, we conduct an ablation where we equalize the maximum correctness and format rewards by setting the former’s range to [−1,1]11[-1,1][ - 1 , 1 ], matching that of the format reward. This adjustment only affects the final normalization step of the correctness reward:\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "|  | ℛcorrect=2⋅RmaxSmax−1∈[−1,1]subscriptℛcorrect⋅2subscript𝑅subscript𝑆111\\mathcal{R}\\_{\\text{correct}}=2\\cdot\\frac{R\\_{\\max}}{S\\_{\\max}}-1\\in[-1,1]caligraphic\\_R start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT = 2 ⋅ divide start\\_ARG italic\\_R start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT end\\_ARG start\\_ARG italic\\_S start\\_POSTSUBSCRIPT roman\\_max end\\_POSTSUBSCRIPT end\\_ARG - 1 ∈ [ - 1 , 1 ] |  |\n",
      "\n",
      "where all variables are defined as in [Section 3.3](https://arxiv.org/html/2504.13958v1#S3.SS3 \"3.3 Reward Design ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\").\n",
      "\n",
      "|  |  |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Non-Live AST Acc | Non-Live Exec Acc | Live Acc | Multi Turn Acc | Relevance Detection | Irrelevance Detection |\n",
      "| Qwen2.5-1.5B-Instruct (Original) | 46.20% | 77.96% | 76.98% | 60.73% | 2.25% | 100.00% | 56.44% |\n",
      "| Qwen2.5-1.5B-Instruct (Equal max) | 39.47% | 78.56% | 75.50% | 45.45% | 2.50% | 100.00% | 16.44% |\n",
      "| Qwen2.5-1.5B-Instruct (Two stage) | 38.85% | 77.96% | 76.23% | 44.51% | 2.25% | 100.00% | 10.61% |\n",
      "| Qwen2.5-1.5B-Instruct (Dynamic) | 45.71% | 78.31% | 75.73% | 58.91% | 2.50% | 100.00% | 57.20% |\n",
      "| Qwen2.5-3B-Instruct (Original) | 52.98% | 81.58% | 79.43% | 73.78% | 3.75% | 88.24% | 84.85% |\n",
      "| Qwen2.5-3B-Instruct (Equal max) | 51.76% | 81.50% | 79.50% | 69.79% | 4.25% | 88.89% | 78.07% |\n",
      "| Qwen2.5-3B-Instruct (Two stage) | 50.66% | 80.62% | 78.82% | 67.93% | 3.50% | 88.89% | 76.42% |\n",
      "| Qwen2.5-3B-Instruct (Dynamic) | 53.81% | 81.44% | 80.75% | 75.43% | 3.62% | 77.78% | 88.82% |\n",
      "| Llama-3.2-3B-Instruct (Original) | 44.10% | 74.38% | 75.18% | 56.86% | 1.37% | 94.44% | 62.23% |\n",
      "| Llama-3.2-3B-Instruct (Equal max) | 42.47% | 67.77% | 75.05% | 55.75% | 1.00% | 88.89% | 59.56% |\n",
      "| Llama-3.2-3B-Instruct (Two stage) | 41.33% | 65.54% | 72.70% | 55.22% | 0.75% | 88.89% | 57.59% |\n",
      "| Llama-3.2-3B-Instruct (Dynamic) | 46.85% | 83.00% | 72.77% | 61.00% | 3.38% | 88.89% | 59.37% |\n",
      "\n",
      "Table 6: BFCL V3 Benchmark Results (Scale)\n",
      "\n",
      "As shown in [Table 6](https://arxiv.org/html/2504.13958v1#S5.T6 \"In 5.2 Effect of Reward Scale ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\"), this equal-scaling variant, denoted as “Equal Max”, results in a slight drop in overall accuracy across most models, with the exception of Qwen2.5-3B, which maintains performance comparable to the original setting. These results underscore the importance of assigning greater weight to correctness reward: doing so helps steer the model toward mastering the core reasoning and tool use capabilities necessary for robust generalization.\n",
      "\n",
      "#### Dynamic Reward Scaling.\n",
      "\n",
      "Building on the insight that correctness reward plays a more critical role, we are further motivated by the intuition that different reward components may benefit from being emphasized at different stages of training. This leads us to explore dynamically adjusting reward scales in accordance with training progress. Specifically, we hypothesize that in early training, the model should prioritize learning the correct output format, which entails an easier objective, before gradually shifting focus to the more challenging goal of tool use correctness. To test this hypothesis, we design two dynamic reward scaling strategies:\n",
      "\n",
      "* •\n",
      "\n",
      "  Two stage (Coarse) Setting: We divide training into two phases. In the first s𝑠sitalic\\_s training steps, we downscale the correctness reward to 1313\\frac{1}{3}divide start\\_ARG 1 end\\_ARG start\\_ARG 3 end\\_ARG of its original scale while keeping the format reward at its original scale. After step s𝑠sitalic\\_s, we restore the correctness reward to its original scale and simultaneously reduce the format reward to range [0,0.5]00.5[0,0.5][ 0 , 0.5 ] (1212\\frac{1}{2}divide start\\_ARG 1 end\\_ARG start\\_ARG 2 end\\_ARG of its original scale). Formally the reward scales are:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | Scaleformat={[0,1]if ⁢Scurrent<s[0,0.5]otherwise,subscriptScaleformatcases01if subscript𝑆current𝑠00.5otherwise\\text{Scale}\\_{\\text{format}}=\\begin{cases}[0,1]&\\text{if }S\\_{\\text{current}}<s% \\\\ [0,0.5]&\\text{otherwise}\\end{cases},Scale start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT = { start\\_ROW start\\_CELL [ 0 , 1 ] end\\_CELL start\\_CELL if italic\\_S start\\_POSTSUBSCRIPT current end\\_POSTSUBSCRIPT < italic\\_s end\\_CELL end\\_ROW start\\_ROW start\\_CELL [ 0 , 0.5 ] end\\_CELL start\\_CELL otherwise end\\_CELL end\\_ROW , |  |\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | Scalecorrect={[−1,1]if ⁢Scurrent<s[−3,3]otherwisesubscriptScalecorrectcases11if subscript𝑆current𝑠33otherwise\\text{Scale}\\_{\\text{correct}}=\\begin{cases}[-1,1]&\\text{if }S\\_{\\text{current}}% <s\\\\ [-3,3]&\\text{otherwise}\\end{cases}Scale start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT = { start\\_ROW start\\_CELL [ - 1 , 1 ] end\\_CELL start\\_CELL if italic\\_S start\\_POSTSUBSCRIPT current end\\_POSTSUBSCRIPT < italic\\_s end\\_CELL end\\_ROW start\\_ROW start\\_CELL [ - 3 , 3 ] end\\_CELL start\\_CELL otherwise end\\_CELL end\\_ROW |  |\n",
      "\n",
      "  where Scurrentsubscript𝑆currentS\\_{\\text{current}}italic\\_S start\\_POSTSUBSCRIPT current end\\_POSTSUBSCRIPT denotes the current training step. In our experiments, we empirically set the switching point to s=30𝑠30s=30italic\\_s = 30 steps, as we observed that the format reward typically experiences a significant increase within the first 30 steps. Therefore, it is more beneficial for later steps to shift focus toward optimizing correctness.\n",
      "* •\n",
      "\n",
      "  Dynamic (Finegrained) Setting: We apply continuous interpolation between the two reward scales throughout training. Initially, both the format and correctness reward scales are set equally. Over time, the format reward scale linearly decays to its original value, while the correctness reward scale gradually increases to its original value, allowing the training to shift focus from format adherence to task correctness accordingly. Formally, the dynamic scaling is then defined as:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | Scaleformat=[−2+p,2−p],subscriptScaleformat2𝑝2𝑝\\text{Scale}\\_{\\text{format}}=[-2+p,2-p],Scale start\\_POSTSUBSCRIPT format end\\_POSTSUBSCRIPT = [ - 2 + italic\\_p , 2 - italic\\_p ] , |  |\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | Scalecorrect=[−2−p,2+p]subscriptScalecorrect2𝑝2𝑝\\text{Scale}\\_{\\text{correct}}=[-2-p,2+p]Scale start\\_POSTSUBSCRIPT correct end\\_POSTSUBSCRIPT = [ - 2 - italic\\_p , 2 + italic\\_p ] |  |\n",
      "\n",
      "  where p∈[0,1]𝑝01p\\in[0,1]italic\\_p ∈ [ 0 , 1 ] similarly represents the normalized training progress. This design ensures a smooth shift of learning focus from format fidelity to correctness.\n",
      "\n",
      "\n",
      "\n",
      "We present the reward dynamics of the original and two dynamic scaling strategies in LABEL:fig:analysis\\_scale\\_dynamic. As shown in [Table 6](https://arxiv.org/html/2504.13958v1#S5.T6 \"In 5.2 Effect of Reward Scale ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\"), the Two stage (Coarse) reward setting unexpectedly leads to a drop in performance, whereas the Dynamic (Finegrained) scaling could improve model’s benchmarking performance. These findings suggest that abrupt shifts in reward scale may negatively impact the training dynamics. In contrast, a smoother and gradual transition from simpler objectives to more nuanced ones appears to better support the model’s learning trajectory and generalization during GRPO training.\n",
      "\n",
      "{bclogo}\n",
      "\n",
      "[couleur=msftBlack!05, epBord=1, arrondi=0.2, logo=\\bcetoile, marge=5, ombre=true, blur, couleurBord=msftBlack!10, tailleOndu=1, sousTitre=Takeaway 2: Gradually adjusting reward scales during training, rather than abrupt changes, better supports model learning and generalization, highlighting the benefits of a smoother transition from simpler objectives to more complex ones.]\n",
      "\n",
      "### 5.3 Effect of Reward Granularity\n",
      "\n",
      "We now perform a detailed analysis of the effect of reward granularity, focusing specifically on the correctness reward. Tool calling, by nature, poses challenges for reward assignment, as it involves multiple facets beyond a single definitive answer (e.g., in contrast to math reasoning tasks). Our original reward design decomposes correctness into matching the tool name, parameter names, and parameter values, offering a finegrained, “process-oriented” signal that reflects partial correctness in tool usage.\n",
      "\n",
      "To assess the impact of this granularity, we evaluate three alternative reward formulations with progressively coarser levels of aggregation:\n",
      "\n",
      "* •\n",
      "\n",
      "  Finegrained: We apply strict exact-match constraints to both tool name and parameter name matching. Specifically, we define:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rname=𝟙⁢[NG=NP]∈{0,1}subscript𝑟name1delimited-[]subscript𝑁𝐺subscript𝑁𝑃01r\\_{\\text{name}}=\\mathds{1}[N\\_{G}=N\\_{P}]\\in\\{0,1\\}italic\\_r start\\_POSTSUBSCRIPT name end\\_POSTSUBSCRIPT = blackboard\\_1 [ italic\\_N start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT = italic\\_N start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ] ∈ { 0 , 1 } |  |\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rparam=∑Gj∈G𝟙⁢[keys⁢(PG)=keys⁢(PP)]∈[0,|G|]subscript𝑟paramsubscriptsubscript𝐺𝑗𝐺1delimited-[]keyssubscript𝑃𝐺keyssubscript𝑃𝑃0𝐺r\\_{\\text{param}}=\\sum\\_{G\\_{j}\\in G}\\mathds{1}[\\text{keys}(P\\_{G})=\\text{keys}(P\\_% {P})]\\in[0,|G|]italic\\_r start\\_POSTSUBSCRIPT param end\\_POSTSUBSCRIPT = ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT blackboard\\_1 [ keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT ) = keys ( italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ) ] ∈ [ 0 , | italic\\_G | ] |  |\n",
      "* •\n",
      "\n",
      "  Intermediate: We combine the parameter name and value rewards into a single term that enforces an exact match on the entire parameter dictionary. Formally:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rparam+rvalue=∑Gj∈G𝟙⁢[PG=PP]∈[0,|G|]subscript𝑟paramsubscript𝑟valuesubscriptsubscript𝐺𝑗𝐺1delimited-[]subscript𝑃𝐺subscript𝑃𝑃0𝐺r\\_{\\text{param}}+r\\_{\\text{value}}=\\sum\\_{G\\_{j}\\in G}\\mathds{1}[P\\_{G}=P\\_{P}]\\in[% 0,|G|]italic\\_r start\\_POSTSUBSCRIPT param end\\_POSTSUBSCRIPT + italic\\_r start\\_POSTSUBSCRIPT value end\\_POSTSUBSCRIPT = ∑ start\\_POSTSUBSCRIPT italic\\_G start\\_POSTSUBSCRIPT italic\\_j end\\_POSTSUBSCRIPT ∈ italic\\_G end\\_POSTSUBSCRIPT blackboard\\_1 [ italic\\_P start\\_POSTSUBSCRIPT italic\\_G end\\_POSTSUBSCRIPT = italic\\_P start\\_POSTSUBSCRIPT italic\\_P end\\_POSTSUBSCRIPT ] ∈ [ 0 , | italic\\_G | ] |  |\n",
      "* •\n",
      "\n",
      "  Coarse: At the coarsest level, we fully entangle tool name, parameter names, and parameter values, treating the entire tool set as a unit. Reward is given only if the generated tool set exactly matches the ground truth:\n",
      "\n",
      "  |  |  |  |\n",
      "  | --- | --- | --- |\n",
      "  |  | rname+rparam+rvalue=𝟙⁢[G=P]∈{0,1}subscript𝑟namesubscript𝑟paramsubscript𝑟value1delimited-[]𝐺𝑃01r\\_{\\text{name}}+r\\_{\\text{param}}+r\\_{\\text{value}}=\\mathds{1}[G=P]\\in\\{0,1\\}italic\\_r start\\_POSTSUBSCRIPT name end\\_POSTSUBSCRIPT + italic\\_r start\\_POSTSUBSCRIPT param end\\_POSTSUBSCRIPT + italic\\_r start\\_POSTSUBSCRIPT value end\\_POSTSUBSCRIPT = blackboard\\_1 [ italic\\_G = italic\\_P ] ∈ { 0 , 1 } |  |\n",
      "\n",
      "All other aspects of reward computation are kept identical to those described in [Section 3.3](https://arxiv.org/html/2504.13958v1#S3.SS3 \"3.3 Reward Design ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\"). Starting from our original design, which is the most finegrained, we progressively entangle reward components to derive increasingly coarse-grained alternatives.\n",
      "\n",
      "![Refer to caption](x2.png)\n",
      "\n",
      "\n",
      "Figure 10: Correctness reward trends across training steps for Qwen2.5-3B-Instruct with different reward granularity.\n",
      "\n",
      "\n",
      "\n",
      "|  |  |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Non-Live AST Acc | Non-Live Exec Acc | Live Acc | Multi Turn Acc | Relevance Detection | Irrelevance Detection |\n",
      "| Qwen2.5-1.5B-Instruct (Original) | 46.20% | 77.96% | 76.98% | 60.73% | 2.25% | 100.00% | 56.44% |\n",
      "| Qwen2.5-1.5B-Instruct (Finegrained) | 40.71% | 78.00% | 75.55% | 48.91% | 2.00% | 100.00% | 24.84% |\n",
      "| Qwen2.5-1.5B-Instruct (Intermediate) | 37.65% | 77.94% | 72.46% | 43.00% | 1.62% | 100.00% | 12.45% |\n",
      "| Qwen2.5-1.5B-Instruct (Coarse) | 36.72% | 76.44% | 70.86% | 41.27% | 2.12% | 100.00% | 12.24% |\n",
      "| Qwen2.5-3B-Instruct (Original) | 52.98% | 81.58% | 79.43% | 73.78% | 3.75% | 88.24% | 84.85% |\n",
      "| Qwen2.5-3B-Instruct (Finegrained) | 52.06% | 81.65% | 79.64% | 69.21% | 5.50% | 83.33% | 78.14% |\n",
      "| Qwen2.5-3B-Instruct (Intermediate) | 51.36% | 81.15% | 80.07% | 68.64% | 4.25% | 88.89% | 75.74% |\n",
      "| Qwen2.5-3B-Instruct (Coarse) | 51.40% | 79.48% | 78.54% | 68.73% | 5.62% | 88.89% | 77.80% |\n",
      "| Llama-3.2-3B-Instruct (Original) | 44.10% | 74.38% | 75.18% | 56.86% | 1.37% | 94.44% | 62.23% |\n",
      "| Llama-3.2-3B-Instruct (Finegrained) | 39.82% | 64.71% | 70.68% | 52.20% | 0.25% | 100.00% | 56.68% |\n",
      "| Llama-3.2-3B-Instruct (Intermediate) | 38.62% | 59.83% | 71.86% | 50.56% | 0.25% | 94.44% | 55.68% |\n",
      "| Llama-3.2-3B-Instruct (Coarse) | 35.95% | 52.00% | 61.43% | 48.96% | 1.12% | 83.33% | 61.92% |\n",
      "\n",
      "Table 7: BFCL V3 Benchmark Results (Granularity)\n",
      "\n",
      "The reward dynamics across training steps, shown in [Figure 10](https://arxiv.org/html/2504.13958v1#S5.F10 \"In 5.3 Effect of Reward Granularity ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\"), demonstrate that as reward granularity becomes coarser, it becomes harder for the model to achieve higher reward values during RL training. This suggests that overly strict and entangled rewards may lead to sparse learning signals, potentially hindering effective credit assignment.\n",
      "\n",
      "Empirical results in [Table 7](https://arxiv.org/html/2504.13958v1#S5.T7 \"In 5.3 Effect of Reward Granularity ‣ 5 Analysis ‣ ToolRL: Reward is All Tool Learning Needs\") further support this insight: our original, most finegrained reward strategy performs well across models. In general, finer-grained reward decomposition leads to better training outcomes and higher final task performance, indicating its advantage in promoting more stable and effective policy learning.\n",
      "\n",
      "{bclogo}\n",
      "\n",
      "[couleur=msftBlack!05, epBord=1, arrondi=0.2, logo=\\bcetoile, marge=5, ombre=true, blur, couleurBord=msftBlack!10, tailleOndu=1, sousTitre=Takeaway 3: Finegrained reward decomposition provides richer learning signals, highlighting its role in enabling more effective training compared to coarse reward formulations, which can impede progress and degrade final performance.]\n",
      "\n",
      "6 Conclusion\n",
      "------------\n",
      "\n",
      "In this paper, we present a reward design tailored for GRPO training on tool use tasks. Empirically, our model trained from scratch using GRPO consistently outperforms both SFT-based and SFT-initialized RL baselines, as well as models trained with alternative RL algorithms, across a variety of held-out tool use benchmarks. Furthermore, we demonstrate that our model generalizes well to QA settings, exhibiting robust multi-turn interactions, emergent proactiveness, and metacognitive behaviors, all of which are key traits for efficient and adaptable tool use, lying at the core of foundational agent capabilities.\n",
      "Our in-depth analysis of reward types, scaling strategies, granularity, and temporal dynamics provides further insights into how reward shaping influences learning and behavior. We hope these findings serve as a roadmap for future work in applying reinforcement learning to tool use. Ultimately, we envision that reward is all tool learning needs, and that RL offers a powerful path toward generalizable and creative agent behavior.\n",
      "\n",
      "References\n",
      "----------\n",
      "\n",
      "* Acikgoz et al. (2025)\n",
      "\n",
      "  Emre Can Acikgoz, Jeremiah Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil Koukoumidis, Dilek Hakkani-Tür, and Gokhan Tur. 2025.\n",
      "  [Can a single model master both multi-turn conversations and tool use? coalm: A unified conversational agentic language model](https://arxiv.org/abs/2502.08820).\n",
      "  *Preprint*, arXiv:2502.08820.\n",
      "* Baek et al. (2024)\n",
      "\n",
      "  Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024.\n",
      "  Researchagent: Iterative research idea generation over scientific literature with large language models.\n",
      "  *arXiv preprint arXiv:2404.07738*.\n",
      "* Chen et al. (2023a)\n",
      "\n",
      "  Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023a.\n",
      "  Fireact: Toward language agent fine-tuning.\n",
      "  *arXiv preprint arXiv:2310.05915*.\n",
      "* Chen et al. (2023b)\n",
      "\n",
      "  Nuo Chen, Hongguang Li, Baoyuan Wang, and Jia Li. 2023b.\n",
      "  From good to great: Improving math reasoning with tool-augmented interleaf prompting.\n",
      "  *arXiv preprint arXiv:2401.05384*.\n",
      "* Chen et al. (2022)\n",
      "\n",
      "  Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022.\n",
      "  Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.\n",
      "  *arXiv preprint arXiv:2211.12588*.\n",
      "* Chen et al. (2024)\n",
      "\n",
      "  Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. 2024.\n",
      "  [Agent-FLAN: Designing data and methods of effective agent tuning for large language models](https://doi.org/10.18653/v1/2024.findings-acl.557).\n",
      "  In *Findings of the Association for Computational Linguistics: ACL 2024*, pages 9354–9366, Bangkok, Thailand. Association for Computational Linguistics.\n",
      "* Chu et al. (2025)\n",
      "\n",
      "  Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V Le, Sergey Levine, and Yi Ma. 2025.\n",
      "  Sft memorizes, rl generalizes: A comparative study of foundation model post-training.\n",
      "  *arXiv preprint arXiv:2501.17161*.\n",
      "* Dang and Ngo (2025)\n",
      "\n",
      "  Quy-Anh Dang and Chris Ngo. 2025.\n",
      "  Reinforcement learning for reasoning in small llms: What works and what doesn’t.\n",
      "  *arXiv preprint arXiv:2503.16219*.\n",
      "* Dubey et al. (2024)\n",
      "\n",
      "  Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024.\n",
      "  The llama 3 herd of models.\n",
      "  *arXiv preprint arXiv:2407.21783*.\n",
      "* Gou et al. (2023)\n",
      "\n",
      "  Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.\n",
      "  Tora: A tool-integrated reasoning agent for mathematical problem solving.\n",
      "  *arXiv preprint arXiv:2309.17452*.\n",
      "* Guo et al. (2025)\n",
      "\n",
      "  Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025.\n",
      "  Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n",
      "  *arXiv preprint arXiv:2501.12948*.\n",
      "* Huang et al. (2023)\n",
      "\n",
      "  Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. 2023.\n",
      "  An embodied generalist agent in 3d world.\n",
      "  *arXiv preprint arXiv:2311.12871*.\n",
      "* Huang et al. (2024)\n",
      "\n",
      "  Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024.\n",
      "  O1 replication journey–part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?\n",
      "  *arXiv preprint arXiv:2411.16489*.\n",
      "* Inoue et al. (2024)\n",
      "\n",
      "  Yoshitaka Inoue, Tianci Song, and Tianfan Fu. 2024.\n",
      "  Drugagent: Explainable drug repurposing agent with large language model-based reasoning.\n",
      "  *arXiv preprint arXiv:2408.13378*.\n",
      "* Jin et al. (2025)\n",
      "\n",
      "  Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025.\n",
      "  Search-r1: Training llms to reason and leverage search engines with reinforcement learning.\n",
      "  *arXiv preprint arXiv:2503.09516*.\n",
      "* Kang et al. (2025)\n",
      "\n",
      "  Minki Kang, Jongwon Jeong, and Jaewoong Cho. 2025.\n",
      "  T1: Tool-integrated self-verification for test-time compute scaling in small language models.\n",
      "  *arXiv preprint arXiv:2504.04718*.\n",
      "* Kaufmann et al. (2023)\n",
      "\n",
      "  Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2023.\n",
      "  A survey of reinforcement learning from human feedback.\n",
      "  *arXiv preprint arXiv:2312.14925*.\n",
      "* Kumar et al. (2025)\n",
      "\n",
      "  Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Salman Khan, and Fahad Shahbaz Khan. 2025.\n",
      "  Llm post-training: A deep dive into reasoning large language models.\n",
      "  *arXiv preprint arXiv:2502.21321*.\n",
      "* Li et al. (2023)\n",
      "\n",
      "  Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023.\n",
      "  Api-bank: A comprehensive benchmark for tool-augmented llms.\n",
      "  In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 3102–3116.\n",
      "* Li et al. (2025a)\n",
      "\n",
      "  Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025a.\n",
      "  Limr: Less is more for rl scaling.\n",
      "  *arXiv preprint arXiv:2502.11886*.\n",
      "* Li et al. (2025b)\n",
      "\n",
      "  Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025b.\n",
      "  Torl: Scaling tool-integrated rl.\n",
      "  *arXiv preprint arXiv:2503.23383*.\n",
      "* Liao et al. (2024)\n",
      "\n",
      "  Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024.\n",
      "  Mario: Math reasoning with code interpreter output–a reproducible pipeline.\n",
      "  *arXiv preprint arXiv:2401.08190*.\n",
      "* Lin et al. (2024)\n",
      "\n",
      "  Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, et al. 2024.\n",
      "  Hammer: Robust function-calling for on-device language models via function masking.\n",
      "  *arXiv preprint arXiv:2410.04587*.\n",
      "* Ling et al. (2023)\n",
      "\n",
      "  Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, et al. 2023.\n",
      "  Domain specialization as the key to make large language models disruptive: A comprehensive survey.\n",
      "  *arXiv preprint arXiv:2305.18703*.\n",
      "* Liu et al. (2024)\n",
      "\n",
      "  Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. 2024.\n",
      "  Toolace: Winning the points of llm function calling.\n",
      "  *arXiv preprint arXiv:2409.00920*.\n",
      "* Meng et al. (2024)\n",
      "\n",
      "  Yu Meng, Mengzhou Xia, and Danqi Chen. 2024.\n",
      "  Simpo: Simple preference optimization with a reference-free reward.\n",
      "  *Advances in Neural Information Processing Systems*, 37:124198–124235.\n",
      "* Patil et al. (2023)\n",
      "\n",
      "  Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023.\n",
      "  Gorilla: Large language model connected with massive apis.\n",
      "  *arXiv preprint arXiv:2305.15334*.\n",
      "* Patil et al. (2024)\n",
      "\n",
      "  Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2024.\n",
      "  Gorilla: Large language model connected with massive apis.\n",
      "  *Advances in Neural Information Processing Systems*, 37:126544–126565.\n",
      "* Press et al. (2022)\n",
      "\n",
      "  Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022.\n",
      "  Measuring and narrowing the compositionality gap in language models.\n",
      "  *arXiv preprint arXiv:2210.03350*.\n",
      "* Qian et al. (2025)\n",
      "\n",
      "  Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025.\n",
      "  Smart: Self-aware agent for tool overuse mitigation.\n",
      "  *arXiv preprint arXiv:2502.11435*.\n",
      "* Qian et al. (2023)\n",
      "\n",
      "  Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023.\n",
      "  Creator: Tool creation for disentangling abstract and concrete reasoning of large language models.\n",
      "  In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages 6922–6939.\n",
      "* Qian et al. (2024a)\n",
      "\n",
      "  Cheng Qian, Peixuan Han, Qinyu Luo, Bingxiang He, Xiusi Chen, Yuji Zhang, Hongyi Du, Jiarui Yao, Xiaocheng Yang, Denghui Zhang, et al. 2024a.\n",
      "  Escapebench: Pushing language models to think outside the box.\n",
      "  *arXiv preprint arXiv:2412.13549*.\n",
      "* Qian et al. (2024b)\n",
      "\n",
      "  Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2024b.\n",
      "  Toolink: Linking toolkit creation and using through chain-of-solving on open-source model.\n",
      "  In *Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*, pages 831–854.\n",
      "* Qin et al. (2024a)\n",
      "\n",
      "  Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. 2024a.\n",
      "  O1 replication journey: A strategic progress report–part 1.\n",
      "  *arXiv preprint arXiv:2410.18982*.\n",
      "* Qin et al. (2023)\n",
      "\n",
      "  Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. 2023.\n",
      "  Tool learning with foundation models.\n",
      "  *arXiv preprint arXiv.2304.08354*, 10.\n",
      "* Qin et al. (2024b)\n",
      "\n",
      "  Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. 2024b.\n",
      "  Tool learning with foundation models.\n",
      "  *ACM Computing Surveys*, 57(4):1–40.\n",
      "* Qin et al. (2024c)\n",
      "\n",
      "  Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024c.\n",
      "  Toolllm: Facilitating large language models to master 16000+ real-world apis.\n",
      "  In *The Twelfth International Conference on Learning Representations*.\n",
      "* Rafailov et al. (2023)\n",
      "\n",
      "  Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2023.\n",
      "  Direct preference optimization: Your language model is secretly a reward model.\n",
      "  *Advances in Neural Information Processing Systems*, 36:53728–53741.\n",
      "* Roohani et al. (2024)\n",
      "\n",
      "  Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, and Jure Leskovec. 2024.\n",
      "  Biodiscoveryagent: An ai agent for designing genetic perturbation experiments.\n",
      "  *arXiv preprint arXiv:2405.17631*.\n",
      "* Schick et al. (2023)\n",
      "\n",
      "  Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\n",
      "  Toolformer: Language models can teach themselves to use tools.\n",
      "  *Advances in Neural Information Processing Systems*, 36:68539–68551.\n",
      "* Schulman et al. (2017)\n",
      "\n",
      "  John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017.\n",
      "  Proximal policy optimization algorithms.\n",
      "  *arXiv preprint arXiv:1707.06347*.\n",
      "* Shao et al. (2024)\n",
      "\n",
      "  Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. 2024.\n",
      "  Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\n",
      "  *arXiv preprint arXiv:2402.03300*.\n",
      "* Shen et al. (2025)\n",
      "\n",
      "  Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. 2025.\n",
      "  Vlm-r1: A stable and generalizable r1-style large vision-language model.\n",
      "  *arXiv preprint arXiv:2504.07615*.\n",
      "* Sheng et al. (2024)\n",
      "\n",
      "  Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024.\n",
      "  Hybridflow: A flexible and efficient rlhf framework.\n",
      "  *arXiv preprint arXiv:2409.19256*.\n",
      "* Song et al. (2025)\n",
      "\n",
      "  Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025.\n",
      "  R1-searcher: Incentivizing the search capability in llms via reinforcement learning.\n",
      "  *arXiv preprint arXiv:2503.05592*.\n",
      "* Team et al. (2025)\n",
      "\n",
      "  Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025.\n",
      "  Kimi k1. 5: Scaling reinforcement learning with llms.\n",
      "  *arXiv preprint arXiv:2501.12599*.\n",
      "* Team (2024)\n",
      "\n",
      "  Qwen Team. 2024.\n",
      "  [Qwen2.5: A party of foundation models](https://qwenlm.github.io/blog/qwen2.5/).\n",
      "* Vu et al. (2023)\n",
      "\n",
      "  Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al. 2023.\n",
      "  Freshllms: Refreshing large language models with search engine augmentation.\n",
      "  *arXiv preprint arXiv:2310.03214*.\n",
      "* Wang et al. (2024)\n",
      "\n",
      "  Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, et al. 2024.\n",
      "  Autosurvey: Large language models can automatically write surveys.\n",
      "  *Advances in Neural Information Processing Systems*, 37:115119–115145.\n",
      "* Xie et al. (2025)\n",
      "\n",
      "  Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. 2025.\n",
      "  Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning.\n",
      "  *arXiv preprint arXiv:2502.14768*.\n",
      "* Yao et al. (2023)\n",
      "\n",
      "  Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023.\n",
      "  React: Synergizing reasoning and acting in language models.\n",
      "  In *The Eleventh International Conference on Learning Representations*.\n",
      "* Ye et al. (2023)\n",
      "\n",
      "  Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2023.\n",
      "  Rational decision-making agent with internalized utility judgment.\n",
      "  *arXiv preprint arXiv:2308.12519*.\n",
      "* Yu et al. (2025)\n",
      "\n",
      "  Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. 2025.\n",
      "  Dapo: An open-source llm reinforcement learning system at scale.\n",
      "  *arXiv preprint arXiv:2503.14476*.\n",
      "* Yu et al. (2024)\n",
      "\n",
      "  Yuanqing Yu, Zhefan Wang, Weizhi Ma, Zhicheng Guo, Jingtao Zhan, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. 2024.\n",
      "  Steptool: A step-grained reinforcement learning framework for tool learning in llms.\n",
      "  *arXiv preprint arXiv:2410.07745*.\n",
      "* Yuan et al. (2025)\n",
      "\n",
      "  Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, et al. 2025.\n",
      "  Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks.\n",
      "  *arXiv preprint arXiv:2504.05118*.\n",
      "* Zeng et al. (2024)\n",
      "\n",
      "  Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang. 2024.\n",
      "  [AgentTuning: Enabling generalized agent abilities for LLMs](https://doi.org/10.18653/v1/2024.findings-acl.181).\n",
      "  In *Findings of the Association for Computational Linguistics: ACL 2024*, pages 3053–3077, Bangkok, Thailand. Association for Computational Linguistics.\n",
      "* Zhai et al. (2024)\n",
      "\n",
      "  Yuanzhao Zhai, Tingkai Yang, Kele Xu, Feng Dawei, Cheng Yang, Bo Ding, and Huaimin Wang. 2024.\n",
      "  Enhancing decision-making for llm agents via step-level q-value models.\n",
      "  *arXiv preprint arXiv:2409.09345*.\n",
      "* Zhang et al. (2023)\n",
      "\n",
      "  Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. 2023.\n",
      "  Building cooperative embodied agents modularly with large language models.\n",
      "  *arXiv preprint arXiv:2307.02485*.\n",
      "* Zhang et al. (2024)\n",
      "\n",
      "  Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. 2024.\n",
      "  xlam: A family of large action models to empower ai agent systems.\n",
      "  *arXiv preprint arXiv:2409.03215*.\n",
      "* Zheng et al. (2025)\n",
      "\n",
      "  Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.\n",
      "  Deepresearcher: Scaling deep research via reinforcement learning in real-world environments.\n",
      "  *arXiv preprint arXiv:2504.03160*.\n",
      "\n",
      "Appendix\n",
      "--------\n",
      "\n",
      "User Prompt for Training\n",
      "\n",
      "Dialogue History\n",
      "  \n",
      "<user> {{ Initial User Input }} </user>\n",
      "<think> Round 1 Model Thought </think>\n",
      "{{ Round 1 model output <tool\\_call> or <response> }}\n",
      "<obs> Round 1 Observation </obs>\n",
      "… …\n",
      "<user> {{ User Input }} </user>\n",
      "… …\n",
      "\n",
      "Figure 11: The user prompt used for TIR’s rollout.\n",
      "\n",
      "Appendix A User Prompt Details\n",
      "------------------------------\n",
      "\n",
      "The system instruction is shown in [Figure 4](https://arxiv.org/html/2504.13958v1#S3.F4 \"In 3.1 Task Definition ‣ 3 Method ‣ ToolRL: Reward is All Tool Learning Needs\"). The user prompt is used to store the trajectory history, including intermediate thoughts, tool calls, environment observations, and any additional user commands. The complete user instruction is presented in [Figure 11](https://arxiv.org/html/2504.13958v1#Ax1.F11 \"In Appendix ‣ ToolRL: Reward is All Tool Learning Needs\").\n",
      "\n",
      "Appendix B Experiment Details\n",
      "-----------------------------\n",
      "\n",
      "#### Training Data Details.\n",
      "\n",
      "We empirically use 4K data points for training, as each dataset consists of samples drawn from the same distribution. Adding more data of similar nature does not increase task diversity. Moreover, we observe that increasing the dataset size beyond 4K does not yield noticeable improvements in the training convergence or final performance, suggesting diminishing returns from additional data under this setting.\n",
      "\n",
      "#### GRPO Setting Details.\n",
      "\n",
      "For all the tool calls in the dataset, we all use JSON format to represent tool call as it’s easy to parse and is the most general and structure way of performing tool call. For the GRPO training, we use 2 A100 (80G) GPUs per run with the following hyper-parameters:\n",
      "\n",
      "|  |  |\n",
      "| --- | --- |\n",
      "| Category | Hyperparameter |\n",
      "| Data Configuration | |\n",
      "| Train Batch Size | 512 |\n",
      "| Validation Batch Size | 128 |\n",
      "| Max Prompt Length | 2048 |\n",
      "| Max Response Length | 1024 |\n",
      "| Optimization | |\n",
      "| Learning Rate | 1e-6 |\n",
      "| PPO Mini Batch Size | 128 |\n",
      "| KL Loss Used | False |\n",
      "| Rollout Configuration | |\n",
      "| Rollout Name | vllm |\n",
      "| GPU Memory Utilization | 0.6 |\n",
      "| Number of Rollouts | 4 |\n",
      "| Training & Logging | |\n",
      "| Save Frequency (Steps) | 15 |\n",
      "| Test Frequency (Steps) | 5 |\n",
      "| Total Epochs | 15 |\n",
      "\n",
      "Table 8: Configuration for GRPO training.\n",
      "\n",
      "#### Baselines.\n",
      "\n",
      "The 400 selected data points used for SFT share the same distribution as the 4k data points used for RL training, but differ in content. For SFT, each data point includes a <think> field, with thought content distilled from Deepseek-R1 trajectories. In contrast, GRPO does not require ground truth thought, as only the tool calls are used to compute rewards in the GRPO setting.\n",
      "\n",
      "We use 400 data points for SFT based on empirical observations that this amount is sufficient to help the raw model learn to follow our tool call format. This provides a stronger initialization and reduces the burden of learning the format from scratch during RL training. However, we also find that relying solely on SFT can lead to overfitting, which may ultimately degrade performance.\n",
      "\n",
      "#### PPO Setting Details.\n",
      "\n",
      "We apply approximately the same parameter settings as GRPO for the PPO training. Similarly, we use 2 A100 (80G) GPUs per run with the following hyper-parameters:\n",
      "\n",
      "|  |  |\n",
      "| --- | --- |\n",
      "| Category | Hyperparameter |\n",
      "| Data Configuration | |\n",
      "| Train Batch Size | 512 |\n",
      "| Validation Batch Size | 128 |\n",
      "| Max Prompt Length | 1024 |\n",
      "| Max Response Length | 512 |\n",
      "| Optimization | |\n",
      "| Actor Learning Rate | 1e-6 |\n",
      "| Critic Learning Rate | 1e-5 |\n",
      "| PPO Mini Batch Size | 128 |\n",
      "| PPO Micro Batch Size | 8 |\n",
      "| KL Coefficient | 0.001 |\n",
      "| Rollout Configuration | |\n",
      "| Rollout Name | vllm |\n",
      "| GPU Memory Utilization | 0.3 |\n",
      "| Training & Logging | |\n",
      "| Critic Warmup Steps | 0 |\n",
      "| Save Frequency (Steps) | 15 |\n",
      "| Test Frequency (Steps) | 5 |\n",
      "| Total Epochs | 15 |\n",
      "\n",
      "Table 9: Configuration for PPO training.\n",
      "\n",
      "Appendix C Additional Results\n",
      "-----------------------------\n",
      "\n",
      "We present additional results on three benchmarks, applying GRPO and PPO methods to models initialized with SFT on 4K data points. This setting serves as a “theoretical” upper bound, since the same 4K data is first used for SFT and subsequently reused for RL training.\n",
      "\n",
      "|  |  |  |  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Non-Live AST Acc | Non-Live Exec Acc | Live Acc | Multi Turn Acc | Relevance Detection | Irrelevance Detection |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 19.41% | 16.00% | 13.18% | 35.58% | 0.00% | 44.44% | 82.49% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 42.95% | 77.65% | 69.75% | 55.73% | 1.88% | 100.00% | 48.40% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 40.93% | 70.54% | 60.79% | 56.33% | 1.00% | 94.44% | 58.63% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+PPO) | 40.24% | 66.42% | 62.02% | 54.58% | 2.50% | 94.12% | 55.09% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+GRPO) | 42.63% | 66.60% | 64.77% | 60.15% | 1.38% | 88.89% | 67.98% |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 33.04% | 42.52% | 40.80% | 53.96% | 1.00% | 64.71% | 56.01% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 45.80% | 78.29% | 71.09% | 58.76% | 5.12% | 94.12% | 54.70% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 46.42% | 76.21% | 68.93% | 64.15% | 1.75% | 88.89% | 71.76% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+PPO) | 48.22% | 77.75% | 73.18% | 64.27% | 5.25% | 94.12% | 66.41% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+GRPO) | 47.82% | 75.12% | 69.52% | 68.19% | 2.38% | 77.78% | 76.16% |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 41.97% | 66.02% | 70.11% | 53.51% | 4.25% | 76.47% | 62.66% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 42.02% | 83.90% | 72.62% | 51.84% | 0.25% | 100% | 29.66% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 39.25% | 80.69% | 74.34% | 46.51% | 0.25% | 100% | 14.19% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+PPO) | 33.80% | 42.67% | 49.50% | 51.80% | 2.38% | 77.78% | 55.79% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+GRPO) | 35.18% | 43.58% | 50.39% | 55.49% | 0.87% | 77.78% | 67.12% |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 22.09% | 17.44% | 14.57% | 43.85% | 0.00% | 77.78% | 66.07% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 41.62% | 68.10% | 69.88% | 52.98% | 3.00% | 94.12% | 56.29% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 42.54% | 65.15% | 68.98% | 59.40% | 0.88% | 72.22% | 65.80% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+PPO) | 45.41% | 73.71% | 68.46% | 62.27% | 2.50% | 82.35% | 68.75% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+GRPO) | 45.50% | 70.69% | 67.70% | 64.73% | 1.00% | 77.78% | 78.85% |\n",
      "\n",
      "Table 10: BFCL V3 Benchmark Results (Additional Result)\n",
      "\n",
      "\n",
      "\n",
      "|  |  |  |  |  |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| Model | Overall Acc | Level 1 | Level 2 | Level 3 |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 30.65% | 28.32% | 35.82% | 35.11% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 57.12% | 60.9% | 50.75% | 48.85% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 61.31% | 64.16% | 58.21% | 54.20% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+PPO) | 61.31% | 64.91% | 56.72% | 52.67% |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+GRPO) | 59.46% | 65.16% | 53.73% | 45.04% |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 51.59% | 59.65% | 32.84% | 36.64% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 65.16% | 67.92% | 55.22% | 61.83% |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 62.48% | 68.67% | 58.21% | 45.80% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+PPO) | 60.13% | 64.41% | 44.78% | 54.96% |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+GRPO) | 60.80% | 64.41% | 56.72% | 51.91% |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 62.48% | 70.68% | 49.25% | 44.27% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 63.15% | 72.43% | 58.21% | 37.4% |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 54.10% | 61.40% | 52.24% | 32.82% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+PPO) | 59.30% | 61.40% | 40.30% | 61.60% |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+GRPO) | 52.60% | 56.39% | 34.33% | 50.38% |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 40.54% | 44.86% | 29.85% | 32.82% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 57.79% | 63.16% | 47.76% | 46.56% |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 56.78% | 63.60% | 41.79% | 43.51% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+PPO) | 54.10% | 60.65% | 40.30% | 41.22% |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+GRPO) | 50.92% | 59.15% | 34.33% | 34.35% |\n",
      "\n",
      "Table 11: API-Bank Test Results (Additional Result)\n",
      "\n",
      "|  |  |  |\n",
      "| --- | --- | --- |\n",
      "| Model | Accuracy | Avg Num Tool Call |\n",
      "| Qwen2.5-1.5B-Instruct (Raw) | 20.8% | 0.61 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+PPO) | 36.8% | 1.06 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT400+GRPO) | 38.4% | 0.96 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+PPO) | 36.8% | 1.06 |\n",
      "| Qwen2.5-1.5B-Instruct (SFT4k+GRPO) | 34.4% | 1.02 |\n",
      "| Qwen2.5-3B-Instruct (Raw) | 52.0% | 1.77 |\n",
      "| Qwen2.5-3B-Instruct (SFT400+PPO) | 43.2% | 1.04 |\n",
      "| Qwen2.5-3B-Instruct (SFT400+GRPO) | 56.8% | 0.99 |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+PPO) | 46.4% | 1.01 |\n",
      "| Qwen2.5-3B-Instruct (SFT4k+GRPO) | 47.2% | 0.98 |\n",
      "| Qwen2.5-7B-Instruct (Raw) | 69.6% | 1.42 |\n",
      "| Qwen2.5-7B-Instruct (SFT400+PPO) | 45.6% | 3.54 |\n",
      "| Qwen2.5-7B-Instruct (SFT400+GRPO) | 29.6% | 3.70 |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+PPO) | 40.0% | 1.25 |\n",
      "| Qwen2.5-7B-Instruct (SFT4k+GRPO) | 32.0% | 1.25 |\n",
      "| Llama-3.2-3B-Instruct (Raw) | 34.4% | 1.25 |\n",
      "| Llama-3.2-3B-Instruct (SFT400+PPO) | 39.2% | 1.33 |\n",
      "| Llama-3.2-3B-Instruct (SFT400+GRPO) | 45.6% | 1.00 |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+PPO) | 49.6% | 1.02 |\n",
      "| Llama-3.2-3B-Instruct (SFT4k+GRPO) | 42.4% | 1.03 |\n",
      "\n",
      "Table 12: Bamboogle Test Results (Additional Result)\n",
      "\n",
      "The results are shown in [Table 10](https://arxiv.org/html/2504.13958v1#A3.T10 \"In Appendix C Additional Results ‣ ToolRL: Reward is All Tool Learning Needs\"), [Table 12](https://arxiv.org/html/2504.13958v1#A3.T12 \"In Appendix C Additional Results ‣ ToolRL: Reward is All Tool Learning Needs\"), and [Table 12](https://arxiv.org/html/2504.13958v1#A3.T12 \"In Appendix C Additional Results ‣ ToolRL: Reward is All Tool Learning Needs\") for BFCL, API-Bank, and Bamboogle, respectively. We compare RL training initialized with models fine-tuned on either 400 or 4K SFT data points.\n",
      "\n",
      "Interestingly, our findings suggest that initializing from a model finetuned on 4K data does not consistently outperform initialization from a model finetuned on only 400 data points. In the BFCL benchmark, we even observe cases where performance drops below that of the raw instruct model. This counterintuitive result may stem from overfitting during the SFT phase, which could restrict the model’s ability to explore during RL training and lead to poorer generalization on held-out tasks.\n",
      "\n",
      "Generated on Wed Apr 16 21:44:27 2025 by [LaTeXML![Mascot Sammy](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==)](http://dlmf.nist.gov/LaTeXML/)\n"
     ]
    }
   ],
   "source": [
    "def extract_content(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetches the content of a given URL and returns it as a markdown page.\n",
    "    \n",
    "    Args:\n",
    "        url: The URL to fetch the content from.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the content of the URL as a markdown page.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    import markdownify\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    markdown = markdownify.markdownify(content)\n",
    "    return markdown\n",
    "\n",
    "# testing the fetch function\n",
    "result = extract_content(\"https://arxiv.org/html/2504.13958v1\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@llm.call(provider='google', model='gemini-2.0-flash', tools=[web_search, extract_content])\n",
    "@prompt_template(        \"\"\"\n",
    "        SYSTEM:\n",
    "        You are an expert web searcher. Your task is to answer the user's question using the provided tools.\n",
    "        Use the current date provided to search the web for the most up to date information.\n",
    "        The current date is {current_date}.\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - `web_search(query: str)`: Searches the web and returns summaries of top results.\n",
    "        - `extract_content(url: str)`: Parse the content of a webpage of a given URL and returns it as a markdown page.\n",
    "\n",
    "        When calling the `web_search` tool, the `body` is simply the summary of the search\n",
    "        result with the URL. You MUST then call the `extract_content` tool to get the actual content\n",
    "        of the webpage. It is up to you to determine which search results to parse.\n",
    "        \n",
    "        You may call one tool per turn, for up to 10 turns before giving your final answer.\n",
    "        \n",
    "        In each turn you should give your thinking process and the final answer when you have gathered all of the information you need.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness based on the context of the user's query.\n",
    "\n",
    "        MESSAGES: {history}\n",
    "        USER: {question}\n",
    "        \"\"\"\n",
    "        )\n",
    "def search(question: str, history: list = None):\n",
    "        return {\"computed_fields\": {\"current_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"history\": history or []}}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleCallResponse(metadata={}, response=GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Okay, I need to find out why the OKC Thunder lost game 1 of the NBA finals. Since the current date is 2025-06-08, I will search for the results of the 2025 NBA finals game 1.\\n'), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'OKC Thunder game 1 2025 NBA finals result'}, name='web_search'), function_response=None, inline_data=None, text=None)], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.07837329412761487, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=76, prompt_token_count=512, total_token_count=588), automatic_function_calling_history=[], parsed=None), tool_types=[<class 'mirascope.core.base._utils._convert_function_to_base_tool.web_search'>, <class 'mirascope.core.base._utils._convert_function_to_base_tool.extract_content'>], prompt_template=\"SYSTEM:\\nYou are an expert web searcher. Your task is to answer the user's question using the provided tools.\\nUse the current date provided to provide the most up to date information.\\nThe current date is {current_date}.\\n\\nYou have access to the following tools:\\n- `web_search(query: str)`: Searches the web and returns summaries of top results.\\n- `extract_content(url: str)`: Parse the content of a webpage of a given URL and returns it as a markdown page.\\n\\nWhen calling the `web_search` tool, the `body` is simply the summary of the search\\nresult with the URL. You MUST then call the `extract_content` tool to get the actual content\\nof the webpage. It is up to you to determine which search results to parse.\\n\\nYou may call one tool per turn, for up to 10 turns before giving your final answer.\\n\\nIn each turn you should give your thinking process and the final answer when you have gathered all of the information you need.\\n\\nOnce you have gathered all of the information you need, generate a writeup that\\nstrikes the right balance between brevity and completeness based on the context of the user's query.\\n\\nMESSAGES: {history}\\nUSER: {question}\", fn_args={'question': 'Why did OKC Thunder lose game 1 of the NBA finals?', 'history': [], 'current_date': '2025-06-08 16:07:54'}, dynamic_config={'computed_fields': {'current_date': '2025-06-08 16:07:54', 'history': []}}, messages=[{'role': 'user', 'parts': [{'text': 'Why did OKC Thunder lose game 1 of the NBA finals?'}, {'text': '\\n\\nFor your final response, output ONLY a valid JSON dict (NOT THE SCHEMA) from the content that adheres to this schema:\\n{\\n  \"properties\": {\\n    \"think\": {\\n      \"description\": \"Your thinking process here\",\\n      \"type\": \"string\"\\n    },\\n    \"final_answer\": {\\n      \"default\": \"\",\\n      \"description\": \"Your final answer here\",\\n      \"type\": \"string\",\\n      \"title\": \"Final Answer\"\\n    }\\n  },\\n  \"required\": [\\n    \"think\"\\n  ],\\n  \"type\": \"object\"\\n}'}]}], call_params={}, call_kwargs={'config': GenerateContentConfig(http_options=None, system_instruction=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"You are an expert web searcher. Your task is to answer the user's question using the provided tools.\\nUse the current date provided to provide the most up to date information.\\nThe current date is 2025-06-08 16:07:54.\\n\\nYou have access to the following tools:\\n- `web_search(query: str)`: Searches the web and returns summaries of top results.\\n- `extract_content(url: str)`: Parse the content of a webpage of a given URL and returns it as a markdown page.\\n\\nWhen calling the `web_search` tool, the `body` is simply the summary of the search\\nresult with the URL. You MUST then call the `extract_content` tool to get the actual content\\nof the webpage. It is up to you to determine which search results to parse.\\n\\nYou may call one tool per turn, for up to 10 turns before giving your final answer.\\n\\nIn each turn you should give your thinking process and the final answer when you have gathered all of the information you need.\\n\\nOnce you have gathered all of the information you need, generate a writeup that\\nstrikes the right balance between brevity and completeness based on the context of the user's query.\")], temperature=None, top_p=None, top_k=None, candidate_count=None, max_output_tokens=None, stop_sequences=None, response_logprobs=None, logprobs=None, presence_penalty=None, frequency_penalty=None, seed=None, response_mime_type=None, response_schema=None, routing_config=None, safety_settings=None, tools=[Tool(function_declarations=[FunctionDeclaration(response=None, description='Searches the web and returns the summaries of top results.\\n\\nReturns:\\n    : A string containing the summaries of the top results.', name='web_search', parameters=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type=<Type.OBJECT: 'OBJECT'>, description=None, enum=None, format=None, items=None, properties={'query': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type=<Type.STRING: 'STRING'>, description='The search query to be executed.', enum=None, format=None, items=None, properties=None, required=None)}, required=['query']))], retrieval=None, google_search=None, google_search_retrieval=None, code_execution=None), Tool(function_declarations=[FunctionDeclaration(response=None, description='Fetches the content of a given URL and returns it as a markdown page.\\n\\nReturns:\\n    : A string containing the content of the URL as a markdown page.', name='extract_content', parameters=Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type=<Type.OBJECT: 'OBJECT'>, description=None, enum=None, format=None, items=None, properties={'url': Schema(min_items=None, example=None, property_ordering=None, pattern=None, minimum=None, default=None, any_of=None, max_length=None, title=None, min_length=None, min_properties=None, max_items=None, maximum=None, nullable=None, max_properties=None, type=<Type.STRING: 'STRING'>, description='The URL to fetch the content from.', enum=None, format=None, items=None, properties=None, required=None)}, required=['url']))], retrieval=None, google_search=None, google_search_retrieval=None, code_execution=None)], tool_config=None, labels=None, cached_content=None, response_modalities=None, media_resolution=None, speech_config=None, audio_timestamp=None, automatic_function_calling=None, thinking_config=None), 'model': 'gemini-2.0-flash', 'contents': [{'role': 'user', 'parts': [{'text': 'Why did OKC Thunder lose game 1 of the NBA finals?'}, {'text': '\\n\\nFor your final response, output ONLY a valid JSON dict (NOT THE SCHEMA) from the content that adheres to this schema:\\n{\\n  \"properties\": {\\n    \"think\": {\\n      \"description\": \"Your thinking process here\",\\n      \"type\": \"string\"\\n    },\\n    \"final_answer\": {\\n      \"default\": \"\",\\n      \"description\": \"Your final answer here\",\\n      \"type\": \"string\",\\n      \"title\": \"Final Answer\"\\n    }\\n  },\\n  \"required\": [\\n    \"think\"\\n  ],\\n  \"type\": \"object\"\\n}'}]}]}, user_message_param={'parts': [{'text': 'Why did OKC Thunder lose game 1 of the NBA finals?'}, {'text': '\\n\\nFor your final response, output ONLY a valid JSON dict (NOT THE SCHEMA) from the content that adheres to this schema:\\n{\\n  \"properties\": {\\n    \"think\": {\\n      \"description\": \"Your thinking process here\",\\n      \"type\": \"string\"\\n    },\\n    \"final_answer\": {\\n      \"default\": \"\",\\n      \"description\": \"Your final answer here\",\\n      \"type\": \"string\",\\n      \"title\": \"Final Answer\"\\n    }\\n  },\\n  \"required\": [\\n    \"think\"\\n  ],\\n  \"type\": \"object\"\\n}'}], 'role': 'user'}, start_time=1749416874475.426, end_time=1749416875267.476, content='Okay, I need to find out why the OKC Thunder lost game 1 of the NBA finals. Since the current date is 2025-06-08, I will search for the results of the 2025 NBA finals game 1.\\n', finish_reasons=['STOP'], model='gemini-2.0-flash', id=None, input_tokens=512, cached_tokens=None, output_tokens=76, cost_metadata=CostMetadata(input_tokens=512, output_tokens=76, cached_tokens=None, streaming_mode=None, cached_response=None, context_length=None, realtime_mode=None, region=None, tier=None, batch_mode=None, images=None, videos=None, audio=None, audio_output=None, pdf=None, context_cache_tokens=None, context_cache_hours=None, google=GoogleMetadata(use_vertex_ai=False, grounding_requests=None), realtime_tokens=None, cache_write=None, tool_use_tokens=None, cost=None), cost=8.16e-05, message_param={'role': 'model', 'parts': [Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Okay, I need to find out why the OKC Thunder lost game 1 of the NBA finals. Since the current date is 2025-06-08, I will search for the results of the 2025 NBA finals game 1.\\n'), Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'query': 'OKC Thunder game 1 2025 NBA finals result'}, name='web_search'), function_response=None, inline_data=None, text=None)]}, thinking=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search(\"Why did OKC Thunder lose game 1 of the NBA finals?\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_with_tools(question: str, max_iterations: int = 10):\n",
    "    \"\"\"\n",
    "    Run the agent with iterative tool calling until completion.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        max_iterations: Maximum number of tool calling iterations to prevent infinite loops\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing the final response and execution details\n",
    "    \"\"\"\n",
    "    conversation_history = []\n",
    "    total_cost = 0\n",
    "    total_tokens = 0\n",
    "    iteration = 0\n",
    "    \n",
    "    print(f\"🤖 Starting agent for question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\n📍 Iteration {iteration}\")\n",
    "        \n",
    "        # Make the LLM call with conversation history\n",
    "        result = search(question, history=conversation_history)\n",
    "        \n",
    "        # making sure you dont go broke\n",
    "        total_cost += result.cost\n",
    "        total_tokens += result.input_tokens + result.output_tokens\n",
    "        \n",
    "        # made the LLM a thinkoor\n",
    "        print(f\"💭 LLM Response: {result.content}\")\n",
    "        \n",
    "        # Add user message to history (this is the agents state)\n",
    "        if iteration == 1 and result.user_message_param:\n",
    "            conversation_history.append(result.user_message_param)\n",
    "        \n",
    "        # Add assistant message to history\n",
    "        conversation_history.append(result.message_param)\n",
    "        \n",
    "        # Check if tools were called\n",
    "        if result.tools:\n",
    "            print(f\"🔧 Tools called: {len(result.tools)}\")\n",
    "            tools_and_outputs = []\n",
    "            \n",
    "            for i, tool in enumerate(result.tools):\n",
    "                print(f\"   Tool {i+1}: {tool._name()}({tool.args})\")\n",
    "                \n",
    "                # Execute the tool\n",
    "                try:\n",
    "                    output = tool.call()\n",
    "                    tools_and_outputs.append((tool, output))\n",
    "                    print(f\"   ✅ Tool output length: {len(str(output))} characters\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Tool error: {e}\")\n",
    "                    tools_and_outputs.append((tool, f\"Error: {e}\"))\n",
    "            \n",
    "            # Add tool results to conversation history\n",
    "            if tools_and_outputs:\n",
    "                conversation_history.extend(\n",
    "                    result.tool_message_params(tools_and_outputs)\n",
    "                )\n",
    "            \n",
    "            # Continue the loop to make another LLM call with the tool results\n",
    "            continue\n",
    "        else:\n",
    "            # No tools called - agent is done\n",
    "            print(\"✅ No tools called - Agent completed!\")\n",
    "            break\n",
    "    \n",
    "    if iteration >= max_iterations:\n",
    "        print(f\"⚠️  Reached maximum iterations ({max_iterations})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"📊 Final Stats:\")\n",
    "    print(f\"   Iterations: {iteration}\")\n",
    "    print(f\"   Total Cost: ${total_cost:.6f}\")\n",
    "    print(f\"   Total Tokens: {total_tokens}\")\n",
    "    print(f\"   Conversation History Length: {len(conversation_history)}\")\n",
    "    \n",
    "    return {\n",
    "        'final_response': result.content,\n",
    "        'iterations': iteration,\n",
    "        'total_cost': total_cost,\n",
    "        'total_tokens': total_tokens,\n",
    "        'conversation_history': conversation_history,\n",
    "        'completed': iteration < max_iterations\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Starting agent for question: Why did OKC Thunder lose game 1 of the NBA finals?\n",
      "============================================================\n",
      "\n",
      "📍 Iteration 1\n",
      "💭 LLM Response: The NBA finals are in June. The current date is June 8, 2025. Therefore, I will search for the results of the 2025 NBA finals.\n",
      "\n",
      "🔧 Tools called: 1\n",
      "   Tool 1: web_search({'query': 'OKC Thunder NBA Finals 2025 game 1 result'})\n",
      "   ✅ Tool output length: 3868 characters\n",
      "\n",
      "📍 Iteration 2\n",
      "💭 LLM Response: \n",
      "🔧 Tools called: 1\n",
      "   Tool 1: extract_content({'url': 'https://www.espn.com/nba/story/_/id/44610574/nba-finals-2025-postseason-news-scores-highlights'})\n",
      "   ✅ Tool output length: 630 characters\n",
      "\n",
      "📍 Iteration 3\n",
      "💭 LLM Response: \n",
      "🔧 Tools called: 1\n",
      "   Tool 1: web_search({'query': 'OKC Thunder vs Pacers Game 1 NBA Finals 2025 recap'})\n",
      "   ✅ Tool output length: 4562 characters\n",
      "\n",
      "📍 Iteration 4\n",
      "💭 LLM Response: The OKC Thunder lost Game 1 of the 2025 NBA Finals to the Indiana Pacers with a score of 111-110. A late comeback by the Pacers, capped by a pull-up jumper from Tyrese Haliburton with 0.3 seconds remaining, secured the victory for Indiana. The Thunder had built a 15-point lead in the fourth quarter but were unable to maintain it.\n",
      "\n",
      "✅ No tools called - Agent completed!\n",
      "\n",
      "============================================================\n",
      "📊 Final Stats:\n",
      "   Iterations: 4\n",
      "   Total Cost: $0.000783\n",
      "   Total Tokens: 7190\n",
      "   Conversation History Length: 8\n",
      "\n",
      "🎯 Final Answer:\n",
      "The OKC Thunder lost Game 1 of the 2025 NBA Finals to the Indiana Pacers with a score of 111-110. A late comeback by the Pacers, capped by a pull-up jumper from Tyrese Haliburton with 0.3 seconds remaining, secured the victory for Indiana. The Thunder had built a 15-point lead in the fourth quarter but were unable to maintain it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with iterative tool calling\n",
    "result = run_agent_with_tools(\"Why did OKC Thunder lose game 1 of the NBA finals?\")\n",
    "\n",
    "print(f\"\\n🎯 Final Answer:\")\n",
    "print(result['final_response'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
